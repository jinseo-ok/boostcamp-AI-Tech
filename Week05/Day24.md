# Week05 - Graph

## [Day 24] - 정점 표현 & 추천시스템

### 1. 그래프 정점의 벡터화

이번 강의에서는 그래프의 정점을 벡터로 표현하는 방법인 정점 임베딩, Node Embedding에 대해서 배웁니다.

기계학습의 다양한 모델은 벡터로 표현된 데이터를 입력으로 받게 됩니다. 그래프를 벡터로 표현할 수 있게 된다면 기계학습의 다양한 모델에 활용 가능하게 됩니다. 그러므로 정점과 정점 사이의 유사성을 벡터로 표현하기 위한 방법은 그래프를 분석하기 위한 첫단계라고 할 수 있습니다.

#### 1) 정점 표현 학습

정점 표현 학습이란 **그래프의 정점들을 벡터의 형태로 표현**하는 것이며, **정점 임베딩, Node Embedding** 이라고도 부릅니다.

정점 임베딩은 벡터 형태의 표현 그 자체를 의미하기도 합니다. 정점이 표현되는 벡터 공간을 임베딩 공간이라도 부릅니다.

임베딩 과정은 주어진 그래프의 각 정점 $u$가 입력으로 이뤄지는 임베딩을 거쳐 벡터 표현 $z_{u}$가 정점 임베딩의 출력이 됩니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109182681-8c8ebe00-77d0-11eb-9f7b-8fe716febdfa.png width = 500>
</center>

정점 임베딩을 하는 이유로는 벡터 형태로 표현된 그래프를 다양한 모델에 적용해 볼 수 있기 때문입니다. 예를 들어, 로지스틱 회귀분석, 다층 퍼셉트론 등과 같은 기계학습의 분류기, 군집 분석 알고리즘에서 요구되는 벡터 형태의 입력의 조건을 만족하게 됩니다. 즉, 그래프의 정점들을 벡터 형태로 표현할 수 있다면, **정점 분류(Node Classification)**, **군집 분석(Community Detection)** 등에 활용할 수 있습니다.

정점 임베딩은 다음 두 단계로 이루어집니다.

(1) 그래프에서의 정점 유사도를 정의하는 단계
(2) 정의한 유사도를 보존하도록 정점 임베딩을 학습하는 단계

정점을 벡터로 변환한다는 의미는 **그래프에서의 정점간 유사도를 임베딩 공간에서도 보존된 상태**임을 말합니다. 그렇다면 그래프에서의 정점간 유사도와 임베딩 공간에서 정점 벡터간 유사도가 유사해야합니다. 이 때, 두 공간에서 유사도를 구하는 방법을 각각 정의해야 합니다.

먼저 임베딩 공간에서의 유사도를 구하기 위해서는 **내적, Inner Product**를 사용합니다. 임베딩 공간에서의 노드 u와 v의 유사도는 둘의 임베딩의 내적은 다음과 같이 구할 수 있습니다. 내적은 두 벡터가 클수록, 그리고 같은 방향을 향할 수록 큰 값을 가지게 됩니다.

$$similarity(u, v) = Z^{t}_{v} Z{u} = ||Z_{u}||⋅||Z_{v}||⋅cos(\theta)$$

다음으로는 그래프에서 정점 유사도를 정의하는 3가지 방법에 대해 알아보겠습니다.

**인접성 기반 접근볍**

**인접성(Adjacency) 기반 접근법** 에서는 두 정점이 인접할 때 유사하다고 간주합니다. 두 정점 𝑢와 𝑣가 인접하다는 것은 둘을 직접 연결하는 간선 (𝑢,𝑣)가 있음을 의미합니다. 인접행렬(Adjacency Matrix) 𝐀의 𝑢행 𝑣열 원소 𝐀𝑢,𝑣는 𝑢와 𝑣가 인접한 경우 1아닌 경우 0입니다 인접행렬의 원소 𝐀𝑢,𝑣 를 두 정점 𝑢와 𝑣의 유사도로 가정합니다.

<image src = https://user-images.githubusercontent.com/48677363/109189830-cc0cd880-77d7-11eb-983b-67fd92156437.png width = 500>

인접성 기반 접근법의 손실 함수(Loss Function)는 다음과 같습니다. 임베딩 공간에서의 유사도와 그래프에서의 유사도의 차이의 제곱을 사용하게 되며, 둘의 차이가 작게 되면 손실 함수는 작아지게 됩니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109189970-f3fc3c00-77d7-11eb-849e-244ebfd918c8.png width = 500>
</center>

하지만 인접성 기반 접근법만으로 유사도를 판단하는 것은 한계가 있습니다. 인접성을 2차원 행렬로 표현하여 유사도를 구하게 되면 단순히 이웃의 경우에만 유사도를 얻을 수 있기 때문에 그래프 상의 군집이나 거리를 고려하지 못한다는 한계를 가지게 됩니다.

**거리 / 경로 / 중첩 기반 접근법**

**거리 기반 접근법**은 두 정점 사이의 거리가 충분히 가까운 경우 유사하다고 간주합니다. 거리 기분을 k로 가정하게 되면, 해당 정점에서 거리 k이하인 정점과의 유사도는 1, 그렇지 않은 정점과의 유사도는 0이 부여됩니다. 다음 그림에서는 거리 기준 2로 정했을 때, 빨간색 정점과의 유사도를 구해볼 수 있습니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109190719-b9df6a00-77d8-11eb-9df0-99443961119d.png width = 200>
</center>

**경로 기반 접근법**은 두 정점 사이의 경로가 많을수록 유사하다고 간주합니다. 두 정점 𝑢와 𝑣의 사이의 경로 중 거리가 𝑘인 것은 수는 $𝐀^𝑘_{𝑢,𝑣}$와 같습니다. 즉, 인접 행렬 𝐀의 𝑘 제곱의 𝑢행 𝑣열 원소와 같습니다. 경로 기반 접근법의 손실 함수는 다음과 같습니다

<center>
<image src = https://user-images.githubusercontent.com/48677363/109192403-80a7f980-77da-11eb-99fa-140547bb96c0.png width = 400>
</center>

**중첩 기반 접근법**에서는 두 정점이 많은 이웃을 공유할수록 유사하다고 간주합니다. 공유하는 이웃의 개수를 유사도로 사용하게 됩니다. 정점 𝑢의 이웃 집합을 𝑁(𝑢) 그리고 정점 𝑣의 이웃 집합을 𝑁(𝑣)라고 하면, 두 정점의 공통 이웃 수 𝑆𝑢,𝑣는 |𝑁𝑢 ∩ 𝑁𝑣|로 정의됩니다. 중첩 기반 접근법의 손실 함수는 다음과 같습니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109192949-33785780-77db-11eb-9081-86bb9ab0160d.png width = 380>
</center>

이 때, 공통 이웃 수 대신 자카드 유사도 혹은 Adamic Adar 점수를 사용할 수도 있습니다.

- 자카드 유사도, Jaccard Similarity는 공통 이웃의 비율을 계산하는 방식입니다. 0과 1 사이의 값을 가지게 되며 이웃이 완벽하게 동일할 때 1이 됩니다.

$$ \frac {공통 이웃의 수}{전체 이웃의 수} = \frac{|N(u) ∩ N(v)|}{|N(u) ∪ N(v)|}$$

- Adamic Adar 점수는 공통 이웃 각각에 가중치를 부여하여 가중합을 계산하는 방식입니다. 연결성을 분모로 하게 됨으로써 연결성이 낮은 정점을 공유하게 되면 높은 가중치를 얻게 됩니다.

$$ \sum_{w\in N(u)∩N(v)} \frac {1}{d_{w}} $$

**임의보행 기반 접근법**

**임의보행 기반 접근법**에서는 한 정점에서 시작하여 임의보행을 할 때 다른 정점에 도달할 확률을 유사도로 간주합니다. 임의보행이란 현재 정점의 이웃 중 하나를 균일한 확률로 선택하여 이동하는 과정을 반복하는 것을 의미합니다. 임의보행을 사용할 경우, 시작 정점 주변의 **지역적 정보**와 **그래프 전역 정보**를 모두 고려한다는 장점이 있습니다. 특히 거리를 k로 제한하는 방식이 아니기 때문에 그래프의 전체적인 정보를 고려할 수 있습니다.

(1) 각 정점에서 시작하여 임의보행을 반복 수행합니다.
(2) 각 정점에서 시작한 임의보행 중 도달한 정점들의 리스틀르 구성합니다. 이 때, 정점 𝑢에서 시작한 임의보행 중 도달한 정점들의 리스트를 $𝑁_r(𝑢)$라고 하며 한 정점을 여러번 도달한 경우, 해당 정점을 중복 포함할 수 있습니다.
(3) 다음 손실함수를 최소화하는 임베딩을 학습합니다. 𝑢에서 𝑣에 도달할 확률의 log 합으로 정의됩니다. 이 때, 도달 확률 자체는 최대화하는 것과 같습니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109195661-227d1580-77de-11eb-855f-8207cfbd179c.png width = 500>
</center>

이 때, 𝑢에서 𝑣에 도달할 확률을 추정하기 위해서 다음 수식을 사용하게 됩니다. 정점 𝑢와 𝑣의 내적의 exponential한 뒤에 정규화를 진행합니다. 즉, 두 정점 간 내적이 크면 클수록 높은 확률(도달 확률)로 추정될 수 있습니다.

$$ P(u|Z_u) = \frac {exp(Z_u^T Z_u)}{\sum_{n \in V}exp(Z_u^T Z_n)} $$

<center>
<image src = https://user-images.githubusercontent.com/48677363/109196626-4725bd00-77df-11eb-9127-3f7d96c18532.png width = 500>
</center>

임의보행의 방법에 따라 **DeepWalk**와 **Node2Vec**으로 구분됩니다.
 
- DeepWalk는 앞서 설명한 기본적인 임의보행을 사용하며, 현재 정점의 이웃 중 하나를 균일한 확률로 선택하여 이동하는 과정을 반복합니다.

- Node2Vec은 2차 치우친 임의보행을 사용합니다.

현재 정점과 직전에 머물렀던 정점을 모두 고려하여 다음 정점을 선택합니다. 직전 정점의 거리를 기준으로 경우를 구분하여 차등적인 확률을 부여합니다. 거리를 기준으로 '거리가 유지되는 방향', '멀어지는 방향', '가까워지는 방향'라는 3가지 확률이 존재하게 됩니다. 차등적인 확률을 사용자가 결정할 수 있으며 부여하는 확률에 따라 다른 임베딩을 얻게 됩니다.

- 멀어지는 방향에 높은 확률을 부여한 경우, 정점의 역할(다리 역할, 변두리 정점 등)이 비슷할 때, 유사한 임베딩이 출력됩니다.
- 가까워지는 방향에 높은 확률을 부여한 경우, 같은 군집에 속할 확률이 높을 때, 유사한 임베딩이 출력됩니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109199249-8d305000-77e2-11eb-9963-56f346f62a89.png width = 500>
</center>

**손실 함수 근사**

임의보행 기법의 손실 함수는 정점의 수의 제곱에 비례하는 비용이 소요됩니다. 모든 정점에 대한 계산이 이뤄지는 부분이 2번이나 나타나기 때문입니다. 정점이 증가할수록 계산에 대한 비용은 기하급수적으로 증가합니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109199469-d7b1cc80-77e2-11eb-8167-db01fe1862a1.png width = 500>
</center>

해당 계산 문제를 해결하기 위해서, 모든 정점에 대한 계산 및 정규화를 하는 대신에 몇 개의 정점을 뽑아서 비교하는 형태를 가지게 됩니다. 이 때, 정점을 뽑는 과정을 **네거티브 샘플링, Negative Sampling** 이라고 합니다.

이 때, 네거티브 샘플을 뽑을 때, 연결성이 높은 정점이 뽑힐 확률이 높으며, 샘플의 수가 많은수록 학습이 더욱 안정적입니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109199828-42fb9e80-77e3-11eb-93a0-3fe66ca7de07.png width = 400>
</center>

#### 2) 변환식 정점 표현 학습의 한계

앞서 배운 **인접성 기반 접근법**, **경로/거리/중첩 기반 접근법**, **임의 보행 기반 접근법** 모두 변환식 정점 표현 학습으로 분류됩니다. 

변환식(Transductive) 방법은 학습의 결과로 정점의 임베딩 값을 얻는다는 특성이 있습니다. 반면에 귀납식(Inductive) 방법은 정점을 임베딩으로 변환하는 함수인 인코더를 출력으로 얻게 됩니다.

변환식 임베딩 방법은 다음과 같은 한계점을 갖습니다.

- 학습이 진행된 이후에 추가된 정점에 대해서는 임베딩을 얻을 수 없음
- 모든 정점에 대한 임베딩을 미리 계산하여 저장해야함
- 정점이 속성(Attribute) 정보를 가진 경우에 이를 활용할 수 없음

-------

### 2. 그래프 기반 추천시스템 II

이번 강의에서는 지난 시간에 배웠던 추천 시스템에 대해 복습하고, 잠재 인수 모형, Latent Factor Model에 대해 배웁니다.

추천 시스템 관련 내용을 공부하게 되면 넷플릭스 추천 알고리즘에 대해 접할 수 밖에 없게 됩니다. 그만큼 넷플릭스는 추천 시스템 분야에서 빼놓을 수 없는 기업 중 하나인데, 그와 관련한 추천 대회인 Netflix Prize에 대해 소개합니다.

추천 시스템의 기초 알고리즘인 content-based filtering과 collaborative filtering 이외의 추천 알고리즘으로 잠재 인수 모형을 활용한 추천 시스템이 존재합니다. 특정 차원에서 단어를 벡터 하나로 나타내는 것처럼, 추천 시스템에서의 사용자와 아이템도 벡터 하나로 표현할 수 있다는 가정에 기반하여 학습이 진행됩니다.

#### 1) 추천 시스템 기본 알고리즘 복습

![image](https://user-images.githubusercontent.com/48677363/109243363-229e0500-7820-11eb-92a2-21ca5133f68b.png)

현재 추천시스템은 정말 다양한 플랫폼에서 사용되고 있습니다. 다양한 플랫폼에서 사용되는 만큼 사용자들에게 영상, 상품, 도서, 호텔 등의 다양한 아이템들을 추천하고 있습니다.

![image](https://user-images.githubusercontent.com/48677363/109243513-6bee5480-7820-11eb-9e09-1b57108e8ba9.png)

추천 시스템에는 다양한 정의가 존재하지만, 추천 시스템의 핵심 목표는 **어떤 사용자에게 어떤 상품을 어떻게 추천**하는 것에 있습니다.

그래프 관점에서 추천 시스템은 **미래의 간선을 예측하는 문제** 혹은 **누락된 간선의 가중치를 추정하는 문제**로 해석할 수 있습니다.

**Content-based Filtering**

![image](https://user-images.githubusercontent.com/48677363/109243786-e323e880-7820-11eb-8a67-c3da5d6302c5.png)

![image](https://user-images.githubusercontent.com/48677363/109244148-8674fd80-7821-11eb-9918-d6b6d66353d7.png)

**Collaborative Filtering**

![image](https://user-images.githubusercontent.com/48677363/109244244-b3c1ab80-7821-11eb-8053-33f4b17d44bc.png)

![image](https://user-images.githubusercontent.com/48677363/109244296-cfc54d00-7821-11eb-93e7-81babf4ca887.png)

**추천 시스템 평가 지표**

![image](https://user-images.githubusercontent.com/48677363/109244471-1c108d00-7822-11eb-873d-ae0ffd28db3b.png)

#### 2) Netflix Prize

[Netflix Challenge](http://www.shalomeir.com/2014/11/netflix-prize-1/)는 넷플릭스에서 주관한 추천 경진대회입니다. 사용자별 영화 평점 데이터가 사용되었으며 훈련 데이터는 2000~2005기간의 약 48만 사용자의 1만8천개 영화의 1억개 평점으로 구성되어 있습니다. 평가 데이터는 각 사용자의 최신 평점 280만개로 구성되어 있습니다.

해당 대회의 목표는 추천 시스템의 성능을 10% 이상 향상시키는 것이었습니다. RMSE를 0.9514에서 0.8563 이하로 낮출 경우 100만불의 상금을 받는 조건이었습니다. 2006~2009년까지 진행되었으며 2700개의 팀이 참여하였습니다. 해당 대회를 통해서 추천 시스템의 성능 및 다양성이 비약적으로 발전하게 되는 계기가 되었습니다.

<image src = https://user-images.githubusercontent.com/48677363/109245047-24b59300-7823-11eb-996e-bcfcc8141678.png width = 400>

#### 3) 잠재 인수 모형, Latent Factor Model

잠재 인수 모형의 핵심은 **사용자와 상품을 벡터로 표현**하는 것입니다. 해당 모형은 uv decomposition 혹은 SVD 라고도 부릅니다.

다음은 사용자와 영화를 2차원 공간에 임베딩한 예시입니다. 2차원 공간에서의 영화와 사용자의 표현을 보게 되면 영화에 대한 사용자의 취향(잠재 변수)에 대해 추측해볼 수 있습니다. 즉, 잠재 인수 모형에서는 사용자와 영화 사이의 가장 효과적인 어떠한 잠재 인수를 학습하는 것을 목표로 합니다.

<image src = https://user-images.githubusercontent.com/48677363/109245384-b8875f00-7823-11eb-9727-5f0ab0919e28.png width = 500>

사용자와 상품을 임베딩하는 기준에 대해서 알아볼 수 있습니다.

- 사용자와 상품의 임베딩의 내적, Inner Product가 평점과 최대한 유사하도록 유도
- 사용자 𝑥의 임베딩을 $p_x$, 상품 𝑖의 임베딩은 $q_i$
- 사용자 𝑥의 상품 𝑖에 대한 평점을 $r_{𝑥𝑖}$
- 임베딩의 목표는 $p_x^T q_i$가 $r_{𝑥𝑖}$과 유사하도록 학습

![image](https://user-images.githubusercontent.com/48677363/109246354-73642c80-7825-11eb-9c46-7d6dff2955f8.png)

![image](https://user-images.githubusercontent.com/48677363/109246462-aad2d900-7825-11eb-80ef-d95dfa1a672d.png)

이 때, 단순 평점 차이에 대한 손실 함수로 학습을 진행하게 되면 **과적합, Overfitting**이 발생하게 됩니다. 과적합을 방지하기 위해서 위와 같이 우항에 **Regularization**을 추가하게 됩니다. 해당 Regularization는 L2-Regularization이며, 실제값과 예측값에 대한 오차 부분의 최소화 뿐만아니라 모형 복잡도 부분도 최소화함으로써 손실 함수를 줄여나갑니다.
극단적인 임베딩 벡터는 기존 데이터의 noise까지 학습하게 됨으로써 추후에 추정하는 테스트 데이터에 generalization된 모델을 제공할 수 없게 됩니다.

<image src = https://user-images.githubusercontent.com/48677363/109247010-8cb9a880-7826-11eb-9edd-940cc3d6695f.png width = 400>

#### 4) 고급 잠재 인수 모형, Latent Factor Model

**사용자와 상품의 편향을 고려한 잠재 인수 모형**

각 사용자의 편향은 해당 사용자의 평점 평균과 전체 평점 평균의 차입니다. 평점의 정도는 정해져있지만 각 사용자마다 평점을 부여하는 기준이 다르기 때문에 발생하는 편향입니다. 

각 상품의 편향은 해당 상품에 대한 평점 평균과 전체 평점 평균의 차입니다. 

이와 같은 편향을 고려하여 개선된 잠재 인수 모형에서는 평점을 **전체 평균, 사용자 편향, 상품 편향, 상호작용**으로 분리합니다. 이전 잠재 인수 모델에서는 단순 상호작용만으로 평점을 예측하는 과정을 가졌다면 개선된 모델에서는 사용자와 상품 편향이 제외된 상호작용을 가지고 예측하는 과정을 가집니다.
 
<image src = https://user-images.githubusercontent.com/48677363/109247483-5d576b80-7827-11eb-88ac-98b6d001532f.png width = 400>

**손실 함수, Loss Function**

편향이 고려된 모델에서는 손실 함수에서도 사용자 편향과 상품 편향 또한 학습하게 됩니다. 추가적으로 우항에는 Regularization을 추가합니다.

<image src = https://user-images.githubusercontent.com/48677363/109247736-c50db680-7827-11eb-9125-d522a5b144bc.png width = 400>

**시간적 편향을 고려한 잠재 인수 모형**

기본 잠재 인수 모델과 편향이 고려된 잠재 인수 모델을 적용함에도 Netflix Prize에서 제시한 10% 이상의 성능을 달성하기에는 부족했습니다. 이 차이를 메우기 위해서 시간적 편향을 고려된 모델이 개발되었습니다. 

2000~20005년이라는 시간이 존재하는 데이터 셋에서 시간적 특징을 고려한 평점 변화도를 보게 되면 다음과 같이 차이가 발생함을 알 수 있습니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109248007-5846ec00-7828-11eb-9e3b-41c43728fe80.png width = 500>
</center>

시간적 편향을 고려하기 위해서는 사용자와 상품의 편향을 고려한 모델에서 사용자 편향과 상품의 편향이 시간적으로 변화할 수 있는 가능성을 추가하게 됩니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109248385-08b4f000-7829-11eb-962b-81b695eb0415.png width = 300>
</center>

최종적으로 Netflix Prize에서는 다음과 같은 결과를 얻을 수 있었습니다.

<image src = https://user-images.githubusercontent.com/48677363/109248473-38fc8e80-7829-11eb-82c1-8cdadb4fcef6.png width = 400>

