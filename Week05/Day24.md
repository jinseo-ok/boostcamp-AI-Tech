# Week05 - Graph

## [Day 24] - 정점 표현 & 추천시스템

### 1. 그래프 정점의 벡터화

이번 강의에서는 그래프의 정점을 벡터로 표현하는 방법인 정점 임베딩, Node Embedding에 대해서 배웁니다.

기계학습의 다양한 모델은 벡터로 표현된 데이터를 입력으로 받게 됩니다. 그래프를 벡터로 표현할 수 있게 된다면 기계학습의 다양한 모델에 활용 가능하게 됩니다. 그러므로 정점과 정점 사이의 유사성을 벡터로 표현하기 위한 방법은 그래프를 분석하기 위한 첫단계라고 할 수 있습니다.

#### 1) 정점 표현 학습

정점 표현 학습이란 **그래프의 정점들을 벡터의 형태로 표현**하는 것이며, **정점 임베딩, Node Embedding** 이라고도 부릅니다.

정점 임베딩은 벡터 형태의 표현 그 자체를 의미하기도 합니다. 정점이 표현되는 벡터 공간을 임베딩 공간이라도 부릅니다.

임베딩 과정은 주어진 그래프의 각 정점 $u$가 입력으로 이뤄지는 임베딩을 거쳐 벡터 표현 $z_{u}$가 정점 임베딩의 출력이 됩니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109182681-8c8ebe00-77d0-11eb-9f7b-8fe716febdfa.png width = 500>
</center>

정점 임베딩을 하는 이유로는 벡터 형태로 표현된 그래프를 다양한 모델에 적용해 볼 수 있기 때문입니다. 예를 들어, 로지스틱 회귀분석, 다층 퍼셉트론 등과 같은 기계학습의 분류기, 군집 분석 알고리즘에서 요구되는 벡터 형태의 입력의 조건을 만족하게 됩니다. 즉, 그래프의 정점들을 벡터 형태로 표현할 수 있다면, **정점 분류(Node Classification)**, **군집 분석(Community Detection)** 등에 활용할 수 있습니다.

정점 임베딩은 다음 두 단계로 이루어집니다.

(1) 그래프에서의 정점 유사도를 정의하는 단계
(2) 정의한 유사도를 보존하도록 정점 임베딩을 학습하는 단계

정점을 벡터로 변환한다는 의미는 **그래프에서의 정점간 유사도를 임베딩 공간에서도 보존된 상태**임을 말합니다. 그렇다면 그래프에서의 정점간 유사도와 임베딩 공간에서 정점 벡터간 유사도가 유사해야합니다. 이 때, 두 공간에서 유사도를 구하는 방법을 각각 정의해야 합니다.

먼저 임베딩 공간에서의 유사도를 구하기 위해서는 **내적, Inner Product**를 사용합니다. 임베딩 공간에서의 노드 u와 v의 유사도는 둘의 임베딩의 내적은 다음과 같이 구할 수 있습니다. 내적은 두 벡터가 클수록, 그리고 같은 방향을 향할 수록 큰 값을 가지게 됩니다.

$$similarity(u, v) = Z^{t}_{v} Z{u} = ||Z_{u}||⋅||Z_{v}||⋅cos(\theta)$$

다음으로는 그래프에서 정점 유사도를 정의하는 3가지 방법에 대해 알아보겠습니다.

**인접성 기반 접근볍**

**인접성(Adjacency) 기반 접근법** 에서는 두 정점이 인접할 때 유사하다고 간주합니다. 두 정점 𝑢와 𝑣가 인접하다는 것은 둘을 직접 연결하는 간선 (𝑢,𝑣)가 있음을 의미합니다. 인접행렬(Adjacency Matrix) 𝐀의 𝑢행 𝑣열 원소 𝐀𝑢,𝑣는 𝑢와 𝑣가 인접한 경우 1아닌 경우 0입니다 인접행렬의 원소 𝐀𝑢,𝑣 를 두 정점 𝑢와 𝑣의 유사도로 가정합니다.

<image src = https://user-images.githubusercontent.com/48677363/109189830-cc0cd880-77d7-11eb-983b-67fd92156437.png width = 500>

인접성 기반 접근법의 손실 함수(Loss Function)는 다음과 같습니다. 임베딩 공간에서의 유사도와 그래프에서의 유사도의 차이의 제곱을 사용하게 되며, 둘의 차이가 작게 되면 손실 함수는 작아지게 됩니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109189970-f3fc3c00-77d7-11eb-849e-244ebfd918c8.png width = 500>
</center>

하지만 인접성 기반 접근법만으로 유사도를 판단하는 것은 한계가 있습니다. 인접성을 2차원 행렬로 표현하여 유사도를 구하게 되면 단순히 이웃의 경우에만 유사도를 얻을 수 있기 때문에 그래프 상의 군집이나 거리를 고려하지 못한다는 한계를 가지게 됩니다.

**거리 / 경로 / 중첩 기반 접근법**

**거리 기반 접근법**은 두 정점 사이의 거리가 충분히 가까운 경우 유사하다고 간주합니다. 거리 기분을 k로 가정하게 되면, 해당 정점에서 거리 k이하인 정점과의 유사도는 1, 그렇지 않은 정점과의 유사도는 0이 부여됩니다. 다음 그림에서는 거리 기준 2로 정했을 때, 빨간색 정점과의 유사도를 구해볼 수 있습니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109190719-b9df6a00-77d8-11eb-9df0-99443961119d.png width = 200>
</center>

**경로 기반 접근법**은 두 정점 사이의 경로가 많을수록 유사하다고 간주합니다. 두 정점 𝑢와 𝑣의 사이의 경로 중 거리가 𝑘인 것은 수는 $𝐀^𝑘_{𝑢,𝑣}$와 같습니다. 즉, 인접 행렬 𝐀의 𝑘 제곱의 𝑢행 𝑣열 원소와 같습니다. 경로 기반 접근법의 손실 함수는 다음과 같습니다

<center>
<image src = https://user-images.githubusercontent.com/48677363/109192403-80a7f980-77da-11eb-99fa-140547bb96c0.png width = 400>
</center>

**중첩 기반 접근법**에서는 두 정점이 많은 이웃을 공유할수록 유사하다고 간주합니다. 공유하는 이웃의 개수를 유사도로 사용하게 됩니다. 정점 𝑢의 이웃 집합을 𝑁(𝑢) 그리고 정점 𝑣의 이웃 집합을 𝑁(𝑣)라고 하면, 두 정점의 공통 이웃 수 𝑆𝑢,𝑣는 |𝑁𝑢 ∩ 𝑁𝑣|로 정의됩니다. 중첩 기반 접근법의 손실 함수는 다음과 같습니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109192949-33785780-77db-11eb-9081-86bb9ab0160d.png width = 380>
</center>

이 때, 공통 이웃 수 대신 자카드 유사도 혹은 Adamic Adar 점수를 사용할 수도 있습니다.

- 자카드 유사도, Jaccard Similarity는 공통 이웃의 비율을 계산하는 방식입니다. 0과 1 사이의 값을 가지게 되며 이웃이 완벽하게 동일할 때 1이 됩니다.

$$ \frac {공통 이웃의 수}{전체 이웃의 수} = \frac{|N(u) ∩ N(v)|}{|N(u) ∪ N(v)|}$$

- Adamic Adar 점수는 공통 이웃 각각에 가중치를 부여하여 가중합을 계산하는 방식입니다. 연결성을 분모로 하게 됨으로써 연결성이 낮은 정점을 공유하게 되면 높은 가중치를 얻게 됩니다.

$$ \sum_{w\in N(u)∩N(v)} \frac {1}{d_{w}} $$

**임의보행 기반 접근법**

**임의보행 기반 접근법**에서는 한 정점에서 시작하여 임의보행을 할 때 다른 정점에 도달할 확률을 유사도로 간주합니다. 임의보행이란 현재 정점의 이웃 중 하나를 균일한 확률로 선택하여 이동하는 과정을 반복하는 것을 의미합니다. 임의보행을 사용할 경우, 시작 정점 주변의 **지역적 정보**와 **그래프 전역 정보**를 모두 고려한다는 장점이 있습니다. 특히 거리를 k로 제한하는 방식이 아니기 때문에 그래프의 전체적인 정보를 고려할 수 있습니다.

(1) 각 정점에서 시작하여 임의보행을 반복 수행합니다.
(2) 각 정점에서 시작한 임의보행 중 도달한 정점들의 리스틀르 구성합니다. 이 때, 정점 𝑢에서 시작한 임의보행 중 도달한 정점들의 리스트를 $𝑁_r(𝑢)$라고 하며 한 정점을 여러번 도달한 경우, 해당 정점을 중복 포함할 수 있습니다.
(3) 다음 손실함수를 최소화하는 임베딩을 학습합니다. 𝑢에서 𝑣에 도달할 확률의 log 합으로 정의됩니다. 이 때, 도달 확률 자체는 최대화하는 것과 같습니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109195661-227d1580-77de-11eb-855f-8207cfbd179c.png width = 500>
</center>

이 때, 𝑢에서 𝑣에 도달할 확률을 추정하기 위해서 다음 수식을 사용하게 됩니다. 정점 𝑢와 𝑣의 내적의 exponential한 뒤에 정규화를 진행합니다. 즉, 두 정점 간 내적이 크면 클수록 높은 확률(도달 확률)로 추정될 수 있습니다.

$$ P(u|Z_u) = \frac {exp(Z_u^T Z_u)}{\sum_{n \in V}exp(Z_u^T Z_n)} $$

<center>
<image src = https://user-images.githubusercontent.com/48677363/109196626-4725bd00-77df-11eb-9127-3f7d96c18532.png width = 500>
</center>

임의보행의 방법에 따라 **DeepWalk**와 **Node2Vec**으로 구분됩니다.
 
- DeepWalk는 앞서 설명한 기본적인 임의보행을 사용하며, 현재 정점의 이웃 중 하나를 균일한 확률로 선택하여 이동하는 과정을 반복합니다.

- Node2Vec은 2차 치우친 임의보행을 사용합니다.

현재 정점과 직전에 머물렀던 정점을 모두 고려하여 다음 정점을 선택합니다. 직전 정점의 거리를 기준으로 경우를 구분하여 차등적인 확률을 부여합니다. 거리를 기준으로 '거리가 유지되는 방향', '멀어지는 방향', '가까워지는 방향'라는 3가지 확률이 존재하게 됩니다. 차등적인 확률을 사용자가 결정할 수 있으며 부여하는 확률에 따라 다른 임베딩을 얻게 됩니다.

- 멀어지는 방향에 높은 확률을 부여한 경우, 정점의 역할(다리 역할, 변두리 정점 등)이 비슷할 때, 유사한 임베딩이 출력됩니다.
- 가까워지는 방향에 높은 확률을 부여한 경우, 같은 군집에 속할 확률이 높을 때, 유사한 임베딩이 출력됩니다.

![image](https://user-images.githubusercontent.com/48677363/109199249-8d305000-77e2-11eb-9963-56f346f62a89.png)

**손실 함수 근사**

임의보행 기법의 손실 함수는 정점의 수의 제곱에 비례하는 비용이 소요됩니다. 모든 정점에 대한 계산이 이뤄지는 부분이 2번이나 나타나기 때문입니다. 정점이 증가할수록 계산에 대한 비용은 기하급수적으로 증가합니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109199469-d7b1cc80-77e2-11eb-8167-db01fe1862a1.png width = 500>
</center>

해당 계산 문제를 해결하기 위해서, 모든 정점에 대한 계산 및 정규화를 하는 대신에 몇 개의 정점을 뽑아서 비교하는 형태를 가지게 됩니다. 이 때, 정점을 뽑는 과정을 **네거티브 샘플링, Negative Sampling** 이라고 합니다.

이 때, 네거티브 샘플을 뽑을 때, 연결성이 높은 정점이 뽑힐 확률이 높으며, 샘플의 수가 많은수록 학습이 더욱 안정적입니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109199828-42fb9e80-77e3-11eb-93a0-3fe66ca7de07.png width = 400>
</center>

#### 2) 변환식 정점 표현 학습의 한계

앞서 배운 **인접성 기반 접근법**, **경로/거리/중첩 기반 접근법**, **임의 보행 기반 접근법** 모두 변환식 정점 표현 학습으로 분류됩니다. 

변환식(Transductive) 방법은 학습의 결과로 정점의 임베딩 값을 얻는다는 특성이 있습니다. 반면에 귀납식(Inductive) 방법은 정점을 임베딩으로 변환하는 함수인 인코더를 출력으로 얻게 됩니다.

변환식 임베딩 방법은 다음과 같은 한계점을 갖습니다.

- 학습이 진행된 이후에 추가된 정점에 대해서는 임베딩을 얻을 수 없음
- 모든 정점에 대한 임베딩을 미리 계산하여 저장해야함
- 정점이 속성(Attribute) 정보를 가진 경우에 이를 활용할 수 없음

-------

### 2. 그래프 기반 추천시스템 II