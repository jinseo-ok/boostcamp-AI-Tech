# Week05 - Graph

## [Day 25] - GNN, Graph Neural Network

### 1. GNN, 그래프 신경망 I

이번 강의에서는 정점 표현 학습, Node Representation Learning 의 방법 중 한 가지인 **그래프 신경망, Graph Neural Network(GNN)** 에 대해서 배웁니다.

#### 1) 정점 표현 학습

정점 표현 학습이란 **그래프의 정점들을 벡터의 형태로 표현**하는 것이며, **정점 임베딩, Node Embedding** 이라고도 부릅니다.

정점 임베딩 방법으로는 크게 **변환식, Transductive** 방식과 **귀납식, Inductive** 방식으로 구분할 수 있습니다.

- 변환식 방법: 학습의 결과로 정점의 임베딩 자체를 출력값으로 얻는 방식
- 귀납식 방법: 학습의 결과로 정점을 임베딩 벡터로 변환하는 함수를 출력값으로 얻는 방식

Day 24에서 중점적으로 알아본 변환식 임베딩 방법은 다음과 같은 **한계점**을 갖습니다.

- 학습이 진행된 이후에 추가된 정점에 대해서는 임베딩을 얻을 수 없음
- 모든 정점에 대한 임베딩을 미리 계산하여 저장해야함(저장해두지 않으면 다시 불러올 수 없기 때문으로 이해했음)
- 정점이 속성(Attribute) 정보를 가진 경우에 이를 활용할 수 없음

**귀납식 임베딩 방법**

반면 출력으로 인코더, 즉 정점을 임베딩 벡터로 변환하는 함수를 출력값으로 얻는 **귀납식 임베딩 방법** 은 다음과 같은 장점을 갖습니다.

- 학습이 진행된 이후에 추가된 정점에 대해서도 임베딩을 얻을 수 있음
- 모든 정점에 대한 임베딩을 미리 계산하여 저장해둘 필요가 없음
- 정점이 속성 정보를 가진 경우에도 이를 활용할 수 있음, 속성 정보를 활용할 수 있도록 함수를 설계할 수 있음

#### 2) GNN 기초

**GNN 기본 구조**

GNN은 그래프와 정점의 속성 정보를 입력으로 받습니다. 그래프는 **인접 행렬 $V * V$의 A**이며, 각 정점 𝑢의 **속성(Attribute) 벡터는 $m$ 차원 벡터 $X_u$** 입니다. 이 때, 정점의 속성의 예시로는 온라인 소셜 네트워크에서 사용자의 **지역, 성별, 연령, 프로필 사진 등**과 논문 인용 그래프에서 사용된 **키워드에 대한 one-hot 벡터** 등이 있습니다.

GNN은 이웃 정점들의 정보를 집계하는 과정을 반복하여 임베딩을 얻습니다. 아래 그림에서는 대상 정점의 임베딩을 얻기 위해 이웃 정점과 이웃의 이웃 정점의 정보를 우측과 같이 도식화할 수 있습니다. 여기서 주의할 점은 2단계 이웃 정점의 정보를 집계하는 과정에서 단계를 무시하고 독립적으로 이웃의 정보를 중복으로 집계했음을 알 수 있습니다.

각 이웃의 정보를 집계하는 단계를 **층, Layer** 라고 하며, 각 층마다 임베딩을 얻습니다. 즉, 이전 층의 이웃 정보를 **집계**하여 임베딩을 얻게 되는데, 0th-layer인 입력층의 임베딩으로는 **정점의 속성 벡터**를 사용합니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109525304-bad40c80-7af4-11eb-9fba-f9493ae4180d.png width = 500>
</center>

결론적으로 임베딩 벡터를 얻고자 하는 대상 정점 마다 이웃의 상태가 다르기 때문에 집계되는 정보가 상이합니다. 이렇게 대상 정점 별 집계되는 구조를 계산 그래프, Compuation Graph라고 합니다.

**집계 함수**

하지만 서로 다른 대상 정점 간에도 층 별 집계 함수는 동일합니다. 여기서 주의해야할 점은 해당 정점이 가지고 있는 **이웃의 크기가 다르기 때문에 각 층별로 공유하는 함수는 가변적인 입력에 대한 처리가 필요**합니다. 

<center>
<image src = https://user-images.githubusercontent.com/48677363/109525796-3afa7200-7af5-11eb-8fb7-8268ea26187f.png width = 500>
</center>

그렇기 때문에 각 층별로 적용되는 집계 함수는 **이웃 정점의 정보의 평균을 계산**하고 **신경망에 적용**하는 단계를 거치게 됩니다. 이 때, 정보의 평균을 계산하는 단계는 가변적인 입력의 크기를 동일하게 만들어주기 위한 과정입니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109527299-d5a78080-7af6-11eb-835a-a96eae7619e4.png width = 500>
</center>

<br>
GNN의 기본적인 구조를 수식적으로 정리하면 다음과 같습니다. input layer에 입력되는 벡터는 이웃 정점의 속성 정보들로 hidden layer와 같은 집계 함수 layer로 입력되게 됩니다. 집계 함수가 존재하는 K개의 layer에서 반복적으로 계산이 진행되며 output layer에서 특정 정점의 임베딩 벡터가 출력됩니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109529182-ce817200-7af8-11eb-85ae-0921e9438667.png width = 700>
</center>

**손실 함수**

GNN의 손실함수의 목표는 **정점 간 거리를 보존하는 것** 입니다. 그래프에서 정점 간 유사도를 계산하는 방법을 정의함에 따라 손실함수 또한 다양한 접근법으로 정의될 수 있습니다. 

아래의 손실함수는 정점 간 유사도를 구하기 위한 인접성 기반 접근법에 대한 손실함수로 Day 24에서 배웠던 손실함수 수식과 동일합니다.

<image src = https://user-images.githubusercontent.com/48677363/109529769-61221100-7af9-11eb-8bab-61594cb8b978.png width = 500>

한편, 후속 과제(Downstream Task)의 손실함수를 이용한 종단종(End-to-End) 학습도 가능합니다.(개인적으로는 임베딩 벡터에 대한 손실함수가 아닌 추후 다른 과제를 위해 정의된 손실함수로 학습이 진행될 수 있음으로 이해했습니다.)

예를 들어, 정점 분류, Node Classification이 최종 과제로 주어졌을 때에는, 임베딩 벡터가 정점 간 유사도를 보존하는 정도가 중요한 것이 아닌 **정점을 정확히 분류하는 정도**가 최종 목표가 될 것입니다.

이 경우에는 분류기의 손실함수인 교차 엔트로피(Cross Entropy)와 같은 손실함수를 전체 학습 과정의 손실함수로 사용할 수 있습니다.

GNN을 End-to-End 학습을 통한 분류는 변환적 정점 임베딩 이후 분류기를 학습하는 것보다 정확도가 대체로 높은 경향을 보였습니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109531303-11444980-7afb-11eb-828b-73eeb77830eb.png width = 550>
</center>

**귀납식 방식인 GNN의 활용**

다시 정리하자면, GNN은 손실함수를 기준으로 loss를 줄여나감으로써 집계가 발생하는 각 layer의 weight를 업데이트함으로써 단순 정점 임베딩 벡터가 아닌 주어진 그래프를 통해 generalization된 임베딩 모델?을 얻을 수 있게 됩니다.

이 때, 학습에 사용할 대상 정점은 모든 정점이 아닌 일부 정점만을 사용할 수 있으며, 선택된 정점들의 계산 그래프를 통해 weight를 업데이트할 수 있습니다. 

- 학습을 통해 만들어진 weight와 설계된 구조로 학습에 사용되지 않은 정점의 임베딩 계산이 가능함

<image src = https://user-images.githubusercontent.com/48677363/109532344-5ddc5480-7afc-11eb-97c4-1287834c4b97.png width = 500>

- 학습 이후에 추가된 정점의 임베딩 계산이 가능함

- 학습된 GNN을 새로운 그래프에 적용 가능하기도함

#### 3) GNN의 변형

앞서 이웃 정점의 정보를 집계하기 위한 집계 함수로 평균을 언급했지만, 다양한 형태의 집계 함수로 변형이 가능합니다. 그리고 접근법의 방식에 따라 정보를 집계하는 특징이 다를 것 입니다.

**그래프 합성곱 신경망, Graph Convolutional Network, GCN**

GCN의 집계 함수를 GNN의 집계 함수와 비교하게 되면 크게 2부분에서 차이가 발생합니다. 수식적으로 형태 자체가 크게 변하지는 않았지만 정보를 집계하는 작은 차이가 큰 성능의 향상으로 이뤄지기도 했습니다.

- 학습 파라미터인 weight 부분입니다. GNN에서는 집계가 이뤄지는 부분과 이전 층의 임베딩에서 각각 parameter가 존재했지만 GCN에서는 하나의 paramter로만 학습이 진행됩니다. 

- 정규화 방법의 변화입니다. GNN에서는 정점 𝑣의 연결성만 고려했다면 GCN에서는 정점 𝑢, 𝑣 연결성의 기하평균을 사용했습니다. 

![image](https://user-images.githubusercontent.com/48677363/109534517-c1678180-7afe-11eb-8b99-da503e5cabd2.png)

**GraphSAGE**

GraphSAGE의 집계 함수는 AGG 함수와 concat에서 차이가 발생합니다.

- 집계 함수로 통과한 벡터와 이전 층의 벡터와 concat하게 된 후에 activation function을 통과하게 됩니다.

- AGG라는 새로운 기능이 추가되었는데 이전 층의 이웃 벡터를 입력함으로써 집계가 이뤄지는 부분입니다. 이 때 다양한 집계 함수가 사용될 수 있습니다.

<center>
<image src = https://user-images.githubusercontent.com/48677363/109535511-017b3400-7b00-11eb-86cc-9e9d3b410b0d.png width = 500>

<image src = https://user-images.githubusercontent.com/48677363/109535696-34252c80-7b00-11eb-9e22-baf2c18b24d0.png width = 350>
</center>

#### 4) CNN과의 비교

CNN 또한 GNN과 유사하게 이웃의 정보를 집계하는 과정을 반복합니다. 대표적으로 CNN은 이미지 처리를 위해 주로 사용되는데, 이웃 픽셀의 정보를 집계하여 이미지의 특징을 파악하는 과정을 반복합니다.

- CNN에서는 입력되는 이웃의 수가 균일하지만, GNN에서는 입력되는 이웃의 수가 가변적임

- CNN의 2차원 픽셀 행렬과 GNN의 2차원 인접 행렬의 의미가 굉장히 다름

    - CNN에서 주로 사용되는 인접 픽셀은 위치적인 인접을 의미하므로 유용한 정보를 담고 있을 가능성이 높음
    - 하지만 그래프의 인접 행렬은 행과 열의 순서가 임의로 결정되는 경우가 많으며 인접 원소 자체가 가지는 정보가 굉장히 제한적임(픽셀은 픽셀 정보를 의미하지만 )



-------

### 2. GNN, 그래프 신경망 II