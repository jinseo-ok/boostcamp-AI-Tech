# Week04 - NLP, Natural Langurage Processing

## [Day 19] - Transformer

### 1. Transformer I

이번 강의에서는 현재 NLP 분야에서 가장 활발히 활용되고 있는 Transforemr에 대해 다루게 됩니다. Transformer 구조의 핵심 매커니즘인 Self-Attention은 RNN 기반 모델의 단점을 해결하기 위해 등장했습니다. RNN과 Attention을 함께 사용했던 기존과는 달리 Attention만을 이용해 입력 문장과 단어의 representation을 학습하면 보다 parallel한 연산이 가능하여 학습 속도가 빠르다는 장점을 보이기도 합니다.





---------

### 2. Transformer II



