{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D25_실습09(using GDL Library(1))",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW394yGalVGt"
      },
      "source": [
        "# 실습 09. \r\n",
        "\r\n",
        "**from dgl.nn import SAGEConv** 를 활용하여 GraphSAGE 모델을 구현하고 학습시켜보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqrq01umVpvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7415b7e6-81c7-493d-936e-da85ba22e793"
      },
      "source": [
        "!pip install dgl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dgl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/b6/5450e9bb80842ab58a6ee8c0da8c7d738465703bceb576bd7e9782c65391/dgl-0.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2yfG0Aj_oej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edfc1c5c-d9b0-4156-8235-e55021f56380"
      },
      "source": [
        "import numpy as np\r\n",
        "import time\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import dgl # Deep Graph Library\r\n",
        "from dgl.data import CoraGraphDataset # Cora 오픈 데이터\r\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y56IETYS-lqg"
      },
      "source": [
        "# 하이퍼파라미터 초기화\r\n",
        "dropoutProb = 0.5\r\n",
        "learningRate = 1e-2\r\n",
        "numEpochs = 50\r\n",
        "numHiddenDim = 128\r\n",
        "numLayers = 2\r\n",
        "weightDecay = 5e-4\r\n",
        "aggregatorType = \"gcn\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_ux5YI1pEh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b6d7c93-4299-4d82-a66f-8c92488de6f2"
      },
      "source": [
        "'''\r\n",
        "    Cora 데이터셋은 2708개의 논문(노드), 10556개의 인용관계(엣지)로 이루어졌습니다. \r\n",
        "    NumFeat은 각 노드를 나타내는 특성을 말합니다. \r\n",
        "    Cora 데이터셋은 각 노드가 1433개의 특성을 가지고, 개개의 특성은 '1'혹은 '0'으로 나타내어지며 특정 단어의 논문 등장 여부를 나타냅니다.\r\n",
        "    즉, 2708개의 논문에서 특정 단어 1433개를 뽑아서, 1433개의 단어의 등장 여부를 통해 각 노드를 표현합니다.\r\n",
        "    \r\n",
        "    노드의 라벨은 총 7개가 존재하고, 각 라벨은 논문의 주제를 나타냅니다\r\n",
        "    [Case_Based, Genetic_Algorithms, Neural_Networks, Probabilistic_Methods, Reinforcement_Learning, Rule_Learning, Theory]\r\n",
        "\r\n",
        "    2708개의 노드 중, 학습에는 140개의 노드를 사용하고 모델을 테스트하는 데에는 1000개를 사용합니다.\r\n",
        "    본 실습에서는 Validation을 진행하지않습니다.\r\n",
        "\r\n",
        "    요약하자면, 앞서 학습시킬 모델은 Cora 데이터셋의 \r\n",
        "    [논문 내 등장 단어들, 논문들 사이의 인용관계]를 활용하여 논문의 주제를 예측하는 모델입니다.\r\n",
        "'''\r\n",
        "\r\n",
        "# Cora Graph Dataset 불러오기\r\n",
        "G = CoraGraphDataset()\r\n",
        "numClasses = G.num_classes # 논문의 7개 주제\r\n",
        "\r\n",
        "G = G[0]\r\n",
        "# 노드들의 feauture & feature의 차원\r\n",
        "features = G.ndata['feat'] # 속성 행렬 node * feature (2708 * 1433)\r\n",
        "inputFeatureDim = features.shape[1] # 1433\r\n",
        "\r\n",
        "# 각 노드들의 실제 라벨\r\n",
        "labels = G.ndata['label']\r\n",
        "\r\n",
        "# 학습/테스트에 사용할 노드들에 대한 표시\r\n",
        "trainMask = G.ndata['train_mask']   \r\n",
        "testMask = G.ndata['test_mask']"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wNucTkIGrBn",
        "outputId": "29f9c955-9ae7-46d4-ee4d-e39571a651f0"
      },
      "source": [
        "trainMask"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ True,  True,  True,  ..., False, False, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfp4dy6TpEfl"
      },
      "source": [
        "# 모델 학습 결과를 평가할 함수\r\n",
        "def evaluateTrain(model, features, labels, mask):\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "        logits = model(features)\r\n",
        "        logits = logits[mask]\r\n",
        "        labels = labels[mask]\r\n",
        "        _, indices = torch.max(logits, dim=1)\r\n",
        "        correct = torch.sum(indices == labels)\r\n",
        "        return correct.item() * 1.0 / len(labels)\r\n",
        "\r\n",
        "def evaluateTest(model, features, labels, mask):\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "        logits = model(features)\r\n",
        "        logits = logits[mask]\r\n",
        "        labels = labels[mask]\r\n",
        "        _, indices = torch.max(logits, dim=1)\r\n",
        "        macro_f1 = f1_score(labels, indices, average = 'macro')\r\n",
        "        correct = torch.sum(indices == labels)\r\n",
        "        return correct.item() * 1.0 / len(labels), macro_f1"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVMJ1qDS84fI"
      },
      "source": [
        "def train(model, lossFunction, features, labels, trainMask, optimizer, numEpochs):\r\n",
        "    executionTime=[]\r\n",
        "    \r\n",
        "    for epoch in range(numEpochs):\r\n",
        "        model.train()\r\n",
        "\r\n",
        "        startTime = time.time()\r\n",
        "            \r\n",
        "        logits = model(features) # 포워딩\r\n",
        "        # mask를 통한 필터링                             \r\n",
        "        loss = lossFunction(logits[trainMask], labels[trainMask])   # 모델의 예측값과 실제 라벨을 비교하여 loss 값 계산\r\n",
        "\r\n",
        "        optimizer.zero_grad()                                       \r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        executionTime.append(time.time() - startTime)\r\n",
        "\r\n",
        "        acc = evaluateTrain(model, features, labels, trainMask)\r\n",
        "\r\n",
        "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f}\".format(epoch, np.mean(executionTime), loss.item(), acc))\r\n",
        "\r\n",
        "def test(model, feautures, labels, testMask):\r\n",
        "    acc, macro_f1 = evaluateTest(model, features, labels, testMask)\r\n",
        "    print(\"Test Accuracy {:.4f}\".format(acc))\r\n",
        "    print(\"Test macro-f1 {:.4f}\".format(macro_f1))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvk8_fKNEZFB"
      },
      "source": [
        "<image src = https://user-images.githubusercontent.com/48677363/109535511-017b3400-7b00-11eb-86cc-9e9d3b410b0d.png width = 700>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYuLoRxfYQyY"
      },
      "source": [
        "# Define GraphSage architecture\r\n",
        "# 기존에 구현되어 있는 SAGEConv 모듈을 불러와서, SAGEConv로 이루어진 GraphSAGE 모델을 구축한다.\r\n",
        "from dgl.nn.pytorch.conv import SAGEConv\r\n",
        "class GraphSAGE(nn.Module):\r\n",
        "    '''\r\n",
        "        graph               : 학습할 그래프\r\n",
        "        inFeatDim           : 데이터의 feature의 차원\r\n",
        "        numHiddenDim        : 모델의 hidden 차원\r\n",
        "        numClasses          : 예측할 라벨의 경우의 수\r\n",
        "        numLayers           : 인풋, 아웃풋 레이어를 제외하고 중간 레이어의 갯수\r\n",
        "        activationFunction  : 활성화 함수의 종류\r\n",
        "        dropoutProb         : 드롭아웃 할 확률\r\n",
        "        aggregatorType      : [mean, gcn, pool (for max), lstm]\r\n",
        "    '''\r\n",
        "    '''\r\n",
        "        SAGEConv(inputFeatDim, outputFeatDim, aggregatorType, dropoutProb, activationFunction)와 같은 형식으로 모듈 생성\r\n",
        "    '''\r\n",
        "    def __init__(self,graph, inFeatDim, numHiddenDim, numClasses, numLayers, activationFunction, dropoutProb, aggregatorType):\r\n",
        "        super(GraphSAGE, self).__init__()\r\n",
        "        self.layers = nn.ModuleList()\r\n",
        "        self.graph = graph\r\n",
        "\r\n",
        "        # 인풋 레이어\r\n",
        "        self.layers.append(\r\n",
        "            SAGEConv(inFeatDim, # 1433\r\n",
        "                     numHiddenDim, # trainable parameter\r\n",
        "                     aggregatorType, # 다양한 집계 함수 지정 가능\r\n",
        "                     dropoutProb, # dropout 확률값\r\n",
        "                     activationFunction\r\n",
        "                     )\r\n",
        "        )\r\n",
        "       \r\n",
        "        # 히든 레이어\r\n",
        "        for i in range(numLayers): # 각 이웃의 정보를 집계하는 단계를 층, Layer라고 하기 때문에 numLayers-2 가 아닌 numLayers 만큼 반복\r\n",
        "            self.layers.append(\r\n",
        "                SAGEConv(\r\n",
        "                    numHiddenDim, # input layer의 output shape은 numHiddenDim 이기 때문\r\n",
        "                    numHiddenDim,\r\n",
        "                    aggregatorType,\r\n",
        "                    dropoutProb,\r\n",
        "                    activationFunction\r\n",
        "                )\r\n",
        "            )\r\n",
        "        \r\n",
        "        # 출력 레이어\r\n",
        "        self.layers.append(SAGEConv(numHiddenDim,\r\n",
        "                                    numClasses, # 출력 클래스 7개(논문 주제)\r\n",
        "                                    aggregatorType,\r\n",
        "                                    dropoutProb,\r\n",
        "                                    activation = None))\r\n",
        "\r\n",
        "    def forward(self, features):\r\n",
        "        x = features\r\n",
        "        for layer in self.layers:\r\n",
        "            x = layer(self.graph, x)\r\n",
        "        return x"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TmJO8orFfn4",
        "outputId": "5467e831-9048-4346-c24f-df6b4df077a0"
      },
      "source": [
        "G, inputFeatureDim, numHiddenDim, numClasses, numLayers, F.relu, dropoutProb, aggregatorType"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Graph(num_nodes=2708, num_edges=10556,\n",
              "       ndata_schemes={'feat': Scheme(shape=(1433,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'h': Scheme(shape=(128,), dtype=torch.float32), 'neigh': Scheme(shape=(128,), dtype=torch.float32)}\n",
              "       edata_schemes={}),\n",
              " 1433,\n",
              " 128,\n",
              " 7,\n",
              " 2,\n",
              " <function torch.nn.functional.relu>,\n",
              " 0.5,\n",
              " 'gcn')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKeX9AdBpJaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb34c95-b10e-43dc-9658-2c24c6fe8e1b"
      },
      "source": [
        "# 모델 생성\r\n",
        "model = GraphSAGE(G, inputFeatureDim, numHiddenDim, numClasses, numLayers, F.relu, dropoutProb, aggregatorType)\r\n",
        "print(model)\r\n",
        "lossFunction = torch.nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "# 옵티마이저 초기화\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GraphSAGE(\n",
            "  (layers): ModuleList(\n",
            "    (0): SAGEConv(\n",
            "      (feat_drop): Dropout(p=0.5, inplace=False)\n",
            "      (fc_neigh): Linear(in_features=1433, out_features=128, bias=True)\n",
            "    )\n",
            "    (1): SAGEConv(\n",
            "      (feat_drop): Dropout(p=0.5, inplace=False)\n",
            "      (fc_neigh): Linear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (2): SAGEConv(\n",
            "      (feat_drop): Dropout(p=0.5, inplace=False)\n",
            "      (fc_neigh): Linear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "    (3): SAGEConv(\n",
            "      (feat_drop): Dropout(p=0.5, inplace=False)\n",
            "      (fc_neigh): Linear(in_features=128, out_features=7, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY9nnzs1pJcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb11ec3-7e23-4cbd-e290-ed9dd1fd14e9"
      },
      "source": [
        "train(model, lossFunction, features, labels, trainMask, optimizer, numEpochs)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 00000 | Time(s) 0.3211 | Loss 1.9561 | Accuracy 0.2857\n",
            "Epoch 00001 | Time(s) 0.2472 | Loss 1.9023 | Accuracy 0.4214\n",
            "Epoch 00002 | Time(s) 0.2134 | Loss 1.8100 | Accuracy 0.5500\n",
            "Epoch 00003 | Time(s) 0.1958 | Loss 1.6103 | Accuracy 0.8143\n",
            "Epoch 00004 | Time(s) 0.1881 | Loss 1.3690 | Accuracy 0.8643\n",
            "Epoch 00005 | Time(s) 0.1822 | Loss 1.0654 | Accuracy 0.9143\n",
            "Epoch 00006 | Time(s) 0.1783 | Loss 0.7656 | Accuracy 0.9143\n",
            "Epoch 00007 | Time(s) 0.1741 | Loss 0.5825 | Accuracy 0.9214\n",
            "Epoch 00008 | Time(s) 0.1711 | Loss 0.3693 | Accuracy 0.9286\n",
            "Epoch 00009 | Time(s) 0.1683 | Loss 0.2876 | Accuracy 0.9500\n",
            "Epoch 00010 | Time(s) 0.1666 | Loss 0.2108 | Accuracy 0.9857\n",
            "Epoch 00011 | Time(s) 0.1648 | Loss 0.1588 | Accuracy 0.9786\n",
            "Epoch 00012 | Time(s) 0.1636 | Loss 0.1437 | Accuracy 0.9857\n",
            "Epoch 00013 | Time(s) 0.1629 | Loss 0.0810 | Accuracy 0.9857\n",
            "Epoch 00014 | Time(s) 0.1622 | Loss 0.0780 | Accuracy 0.9786\n",
            "Epoch 00015 | Time(s) 0.1614 | Loss 0.1049 | Accuracy 0.9857\n",
            "Epoch 00016 | Time(s) 0.1606 | Loss 0.0487 | Accuracy 0.9857\n",
            "Epoch 00017 | Time(s) 0.1597 | Loss 0.0511 | Accuracy 0.9929\n",
            "Epoch 00018 | Time(s) 0.1589 | Loss 0.0574 | Accuracy 0.9929\n",
            "Epoch 00019 | Time(s) 0.1583 | Loss 0.0164 | Accuracy 1.0000\n",
            "Epoch 00020 | Time(s) 0.1578 | Loss 0.0358 | Accuracy 1.0000\n",
            "Epoch 00021 | Time(s) 0.1572 | Loss 0.0236 | Accuracy 0.9857\n",
            "Epoch 00022 | Time(s) 0.1568 | Loss 0.0778 | Accuracy 0.9857\n",
            "Epoch 00023 | Time(s) 0.1568 | Loss 0.0406 | Accuracy 0.9857\n",
            "Epoch 00024 | Time(s) 0.1564 | Loss 0.0583 | Accuracy 1.0000\n",
            "Epoch 00025 | Time(s) 0.1560 | Loss 0.0123 | Accuracy 0.9929\n",
            "Epoch 00026 | Time(s) 0.1557 | Loss 0.0350 | Accuracy 1.0000\n",
            "Epoch 00027 | Time(s) 0.1553 | Loss 0.0167 | Accuracy 0.9929\n",
            "Epoch 00028 | Time(s) 0.1550 | Loss 0.0206 | Accuracy 0.9786\n",
            "Epoch 00029 | Time(s) 0.1547 | Loss 0.0244 | Accuracy 0.9857\n",
            "Epoch 00030 | Time(s) 0.1542 | Loss 0.0426 | Accuracy 1.0000\n",
            "Epoch 00031 | Time(s) 0.1538 | Loss 0.0149 | Accuracy 0.9929\n",
            "Epoch 00032 | Time(s) 0.1537 | Loss 0.0250 | Accuracy 0.9857\n",
            "Epoch 00033 | Time(s) 0.1537 | Loss 0.0456 | Accuracy 0.9929\n",
            "Epoch 00034 | Time(s) 0.1535 | Loss 0.0843 | Accuracy 1.0000\n",
            "Epoch 00035 | Time(s) 0.1533 | Loss 0.0298 | Accuracy 0.9929\n",
            "Epoch 00036 | Time(s) 0.1533 | Loss 0.0202 | Accuracy 1.0000\n",
            "Epoch 00037 | Time(s) 0.1532 | Loss 0.0238 | Accuracy 1.0000\n",
            "Epoch 00038 | Time(s) 0.1531 | Loss 0.0147 | Accuracy 1.0000\n",
            "Epoch 00039 | Time(s) 0.1530 | Loss 0.0375 | Accuracy 1.0000\n",
            "Epoch 00040 | Time(s) 0.1528 | Loss 0.0329 | Accuracy 1.0000\n",
            "Epoch 00041 | Time(s) 0.1527 | Loss 0.0343 | Accuracy 0.9929\n",
            "Epoch 00042 | Time(s) 0.1525 | Loss 0.0551 | Accuracy 1.0000\n",
            "Epoch 00043 | Time(s) 0.1524 | Loss 0.0562 | Accuracy 0.9857\n",
            "Epoch 00044 | Time(s) 0.1523 | Loss 0.0439 | Accuracy 0.9857\n",
            "Epoch 00045 | Time(s) 0.1522 | Loss 0.0743 | Accuracy 1.0000\n",
            "Epoch 00046 | Time(s) 0.1519 | Loss 0.0288 | Accuracy 0.9929\n",
            "Epoch 00047 | Time(s) 0.1517 | Loss 0.0634 | Accuracy 0.9929\n",
            "Epoch 00048 | Time(s) 0.1516 | Loss 0.0219 | Accuracy 0.9929\n",
            "Epoch 00049 | Time(s) 0.1515 | Loss 0.0239 | Accuracy 0.9786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-swaKM7E-KiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5786b8d6-4374-4b05-a889-25df2f3ff4fc"
      },
      "source": [
        "test(model, features, labels, testMask)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.7690\n",
            "Test macro-f1 0.7620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5E2HkTNA6DR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}