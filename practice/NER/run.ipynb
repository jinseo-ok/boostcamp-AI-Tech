{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fd9e889ea21e468ea6ea7d0ce5a4d7b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_107c79cb340347ae8ee3cc1838647e43",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_280ae71c74e54f9b993c5fbb435a7dc7",
              "IPY_MODEL_0929226c48454c808012ac37a4a18588"
            ]
          }
        },
        "107c79cb340347ae8ee3cc1838647e43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "280ae71c74e54f9b993c5fbb435a7dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4819ddbc7f8541e29da2c5cad8a9a0fa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dc7a1c9a56f6406ab0a6870df77179e6"
          }
        },
        "0929226c48454c808012ac37a4a18588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e74424b2ecce4c89bfbb452f0a679855",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7000/? [11:03&lt;00:00, 10.55it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bba48383346e4f7584c9ad7e331b0f4f"
          }
        },
        "4819ddbc7f8541e29da2c5cad8a9a0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dc7a1c9a56f6406ab0a6870df77179e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e74424b2ecce4c89bfbb452f0a679855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bba48383346e4f7584c9ad7e331b0f4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWQQVPeY0xch",
        "outputId": "676466e6-627f-44a5-a7de-02cb4b22bfd0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AY0e-g7-KD8Fn1zBOXIP00hoAb6SxnnIa6DiofzFIDI2R42F1lKfiUQrMqg\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwT6qoeH05IA",
        "outputId": "32fd0203-eac3-4630-be87-1e3adb8d827b"
      },
      "source": [
        "cd /content/drive/MyDrive/boostcamp/NER"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/boostcamp/NER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdKLc4YaIXow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf53dc6c-997f-4ca7-833b-29568cfc1b29"
      },
      "source": [
        "#!pip install torch==1.6.0\n",
        "!pip install transformers==3.3.1\n",
        "!pip install seqeval\n",
        "!pip install fastprogress\n",
        "!pip install attrdict"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 615kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (4.41.1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 6.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.3.1) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=03fde2337658de6e32c1f05d2822c0bb9d337ef9f2394ae652775ef4c2d8cc67\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.8.1rc2 transformers-3.3.1\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 515kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=8136577c3873340cdd5a6a78bd003c978b4aca9bb51df206f3cbca4a835ef9c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastprogress) (1.19.5)\n",
            "Collecting attrdict\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/97/28fe7e68bc7adfce67d4339756e85e9fcf3c6fd7f0c0781695352b70472c/attrdict-2.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from attrdict) (1.15.0)\n",
            "Installing collected packages: attrdict\n",
            "Successfully installed attrdict-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnLj4NEYNOyx"
      },
      "source": [
        "해당 과제의 코드는 박장원님의 \"https://github.com/monologg/KoELECTRA/tree/master/finetune\" 레포지토리의 코드를 변경하여 만들어졌습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fzRvsLRNXhj"
      },
      "source": [
        "추신. 좋은 코드 만들어주시고, 코드 사용을 흔쾌히 허락해주신 박장원님께 감사인사 드립니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXxx9lqUMYOH"
      },
      "source": [
        "학습을 시작하기 전에 같이 동봉된 \"preprocessing.ipynb\"를 실행하여 나오는 \"train.tsv\",\"dev.tsv\"를 \"data/custom-ner\"에 넣어줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoSoMGt3ISc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7f802f-290c-473a-eae9-61ac0ce450fe"
      },
      "source": [
        "!python3 run_ner.py --task custom-ner --config_file koelectra-small.json # config/custom-ner 에서 원하는 모델의 config를 변경하고 넣어주세요."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-19 15:08:36.738371: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/19/2021 15:08:49 - INFO - filelock -   Lock 140579867383680 acquired on /root/.cache/torch/transformers/8da255e6abb41f960d45d41af1e9efda71eb73e1fbbd1091b63b443888a1c3a3.73a6db14d6ede1dd8840a057bd410929cf4f97dc456644ca8b80b76f09166942.lock\n",
            "Downloading: 100% 466/466 [00:00<00:00, 323kB/s]\n",
            "02/19/2021 15:08:50 - INFO - filelock -   Lock 140579867383680 released on /root/.cache/torch/transformers/8da255e6abb41f960d45d41af1e9efda71eb73e1fbbd1091b63b443888a1c3a3.73a6db14d6ede1dd8840a057bd410929cf4f97dc456644ca8b80b76f09166942.lock\n",
            "02/19/2021 15:08:51 - INFO - filelock -   Lock 140578720632336 acquired on /root/.cache/torch/transformers/825effcd21ded11fe1adfedb39968a0a60354b9c2e6bc769c36543f7c5f3c541.79565c1f8213906e95b08997d8e3f41e4b5ef049ffab0e770dfe3dea3dce562d.lock\n",
            "Downloading: 100% 279k/279k [00:00<00:00, 390kB/s] \n",
            "02/19/2021 15:08:52 - INFO - filelock -   Lock 140578720632336 released on /root/.cache/torch/transformers/825effcd21ded11fe1adfedb39968a0a60354b9c2e6bc769c36543f7c5f3c541.79565c1f8213906e95b08997d8e3f41e4b5ef049ffab0e770dfe3dea3dce562d.lock\n",
            "02/19/2021 15:08:54 - INFO - filelock -   Lock 140578720632336 acquired on /root/.cache/torch/transformers/7995007ef27cce70bf9876d2243762a006c9bd057434e4ab8205aecc6c014f24.f823277c1796df7b9584d6424272b3cfa2a493c007b227382c479e47ef12b985.lock\n",
            "Downloading: 100% 51.0/51.0 [00:00<00:00, 40.4kB/s]\n",
            "02/19/2021 15:08:55 - INFO - filelock -   Lock 140578720632336 released on /root/.cache/torch/transformers/7995007ef27cce70bf9876d2243762a006c9bd057434e4ab8205aecc6c014f24.f823277c1796df7b9584d6424272b3cfa2a493c007b227382c479e47ef12b985.lock\n",
            "02/19/2021 15:08:56 - INFO - filelock -   Lock 140578721842344 acquired on /root/.cache/torch/transformers/c07f693f3aebfc37406b3300e3383ce3b6b8f3bf61b84a907cf38ed9869bddef.350a226fe08b1e0a74c700df8e738f55e4b510aca5b22231e702bdec9de0ce2f.lock\n",
            "Downloading: 100% 55.1M/55.1M [00:00<00:00, 59.0MB/s]\n",
            "02/19/2021 15:08:57 - INFO - filelock -   Lock 140578721842344 released on /root/.cache/torch/transformers/c07f693f3aebfc37406b3300e3383ce3b6b8f3bf61b84a907cf38ed9869bddef.350a226fe08b1e0a74c700df8e738f55e4b510aca5b22231e702bdec9de0ce2f.lock\n",
            "Some weights of the model checkpoint at monologg/koelectra-small-discriminator were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
            "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-small-discriminator and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "02/19/2021 15:09:10 - INFO - processor.ner -   Creating features from dataset file at data\n",
            "02/19/2021 15:09:10 - INFO - processor.ner -   LOOKING AT data/custom-ner/train.tsv\n",
            "02/19/2021 15:09:12 - INFO - processor.ner -   네 여러분 안녕하십니까. 엄길청입니다.\tO O O PS-B\n",
            "02/19/2021 15:09:12 - INFO - processor.ner -   국립 공원을 여기저기 그냥 지정하는 것으로 하지 말고\tO O O O O O O O\n",
            "02/19/2021 15:09:12 - INFO - processor.ner -   근데 저 개인적으로는 땅콩소스를 싫어해요.\tO O O O O\n",
            "02/19/2021 15:09:12 - INFO - processor.ner -   그 공간이 따로 있을 정도라고 해요.\tO O O O O O\n",
            "02/19/2021 15:09:12 - INFO - processor.ner -   이 교과서를 정부가 포기하지 않는다면\tO O OG-B O O\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   또 뭔가 예의 바르고 어~ 정말 별 많은 나라로\tO O O O O O O O O\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   놨겠네 일주일 계시다가 나왔으니\tO DT-B O O\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   다운그레이드 되셨습니다\tO O\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   아니 근데도 굉장히 말라 말랐는데\tO O O O O\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   마이크에 대고.\tO O\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   사회에 던지신 바가 있습니다만\tO O O O\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   네.\tO\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   불만 부분도 어~\tO O O\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   그래서 그냥\tO O\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   지금 전화를 해서 지금이라도 전화 근데 전화를 하면 안 받을 것 같았고 이미 늦었고\tO O O O O O O O O O O O O O\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   네 또 질문하실 분 계신가요.\tO O O O O\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   네. 이제 마늘 다짐 너 줘야지 맛있겠죠?\tO O O O O O O\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   뭐라도 좀 힐링을 해야겠다 자체적으로\tO O O O O\n",
            "02/19/2021 15:09:13 - INFO - processor.ner -   박근혜 영애한테\tPS-B O\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   네.\tO\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   아몬드 넣고 뭐\tO O O\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   한국문학연구가 문학의 에 본래로 돌아가기 위해선\tO O O O O O\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   아주 철저하게 하고 있습니다 그래서\tO O O O O\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   카타르월드컵 개최 시기는 새달 19일 스위스 취리히에서 열리는 피파 집행위원회에서 최종 확정된다.\tO O O DT-B DT-I LC-B LC-B O OG-B O O O\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   중국은 이번 조처를 통해서 미국 등이 요구하던 시장에 환율을 맡기는 변동환율제로 한걸음 더 나아가는 명분을 얻었다.\tOG-B O O O OG-B O O O O O O QT-B O O O O\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   일반학기(필수)에 방학기간인 6주 동안 집중학기(선택)를 더해 학점을 취득하는 방식으로 조기졸업이 가능하다.\tO O DT-B DT-I O O O O O O O\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   4월 장익제를 시작으로 이경훈 김형성 김경태 류현우 등이 차례로 우승컵을 들어올렸다.\tDT-B PS-B O PS-B PS-B PS-B PS-B O O O O\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   100㎡(30평) 남짓한 실내에 26명의 수강생이 빼곡히 들어앉아 진지한 표정으로 강사의 설명에 귀를 기울였다.\tQT-B O O QT-B O O O O O O O O O\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   반면 고용조정이 불가피한 사업주가 휴업·훈련 등을 통해 노동자의 고용을 유지하면 지급하는 고용유지지원금 등 고용 관련 다른 지표는 호전된 것으로 나타났다.\tO O O O O O O O O O O O O O O O O O O O\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   동등 지분 확보 ‘기술먹튀’ 방지 의도\tO O O O O O\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   '스두'와 '쿵차오'란 뭘까.\tO O O\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   1943년 체코 법원의 궐석재판에서 사형이 선고된 그는 캐나다로 도주해 수십년간 가명으로 예술품 거래상을 하다 1995년 신분이 탄로나 다시 도망자 신세가 됐다.\tDT-B OG-B OG-I O O O O LC-B O O O O O O DT-B O O O O O O\n",
            "02/19/2021 15:09:14 - INFO - processor.ner -   회사와 쌍용차지부는 1시간가량 이어진 교섭이 끝난 뒤 정리해고 사태를 해결하기 위한 “4대 의제를 확정하고 실무교섭을 빠르게 진행하기로 합의했다”고 밝혔다.\tO OG-B TI-B O O O O O O O O O O O O O O O O\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   범정부사고대책본부 관계자는 25일 <한겨레>와 한 통화에서 “언딘의 활동이 선박 인양이 주목적인 것은 맞다.\tOG-B O DT-B O O O O O O O O O O\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   Writing example 0 of 334166\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   *** Example ***\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   guid: train-0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   tokens: [CLS] 네 여러분 안녕 ##하 ##십니까 . 엄 ##길 ##청 ##입니다 . [SEP]\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   input_ids: 2 843 17067 19864 5803 21624 18 3456 5756 5736 10621 18 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   label: -100 12 12 12 -100 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   *** Example ***\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   guid: train-1\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   tokens: [CLS] 마냥 행복 ##하고 사랑 ##만 느끼고 싶은 계절 오 ##월 ##입니다 . [SEP]\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   input_ids: 2 25691 12310 10546 11277 5666 18599 13051 18424 3535 6179 10621 18 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   label: -100 12 12 -100 12 -100 12 12 12 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   *** Example ***\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   guid: train-2\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   tokens: [CLS] 가정의 달 ##도 오 ##월 ##이 ##죠 . 사소한 말 끝에 큰 싸움이 나 ##기도 하 ##구요 . [SEP]\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   input_ids: 2 24711 1053 5668 3535 6179 5706 6183 18 26492 1903 13067 4816 26124 757 10674 5346 12667 18 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   label: -100 12 12 -100 0 -100 -100 -100 -100 12 12 12 12 12 12 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   *** Example ***\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   guid: train-3\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   tokens: [CLS] 또 법 ##보다 주먹 ##이 가깝다 ##는 생각도 가끔 들 때가 있습니다 . [SEP]\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   input_ids: 2 1462 2292 10625 18341 5706 23996 5735 24425 14645 1313 16007 10668 18 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   label: -100 12 12 -100 12 -100 12 -100 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   *** Example ***\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   guid: train-4\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   tokens: [CLS] 학교폭력 ##도 따 ##지고 보면 말 때문에 일어나는 일이 많지 않을까 싶습니다 . [SEP]\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   input_ids: 2 28264 5668 1359 10679 11046 1903 10603 19423 11373 17651 16227 20204 18 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:09:15 - INFO - processor.ner -   label: -100 12 -100 12 -100 12 12 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/19/2021 15:09:17 - INFO - processor.ner -   Writing example 10000 of 334166\n",
            "02/19/2021 15:09:19 - INFO - processor.ner -   Writing example 20000 of 334166\n",
            "02/19/2021 15:09:21 - INFO - processor.ner -   Writing example 30000 of 334166\n",
            "02/19/2021 15:09:23 - INFO - processor.ner -   Writing example 40000 of 334166\n",
            "02/19/2021 15:09:26 - INFO - processor.ner -   Writing example 50000 of 334166\n",
            "02/19/2021 15:09:29 - INFO - processor.ner -   Writing example 60000 of 334166\n",
            "02/19/2021 15:09:31 - INFO - processor.ner -   Writing example 70000 of 334166\n",
            "02/19/2021 15:09:34 - INFO - processor.ner -   Writing example 80000 of 334166\n",
            "02/19/2021 15:09:37 - INFO - processor.ner -   Writing example 90000 of 334166\n",
            "02/19/2021 15:09:40 - INFO - processor.ner -   Writing example 100000 of 334166\n",
            "02/19/2021 15:09:42 - INFO - processor.ner -   Writing example 110000 of 334166\n",
            "02/19/2021 15:09:45 - INFO - processor.ner -   Writing example 120000 of 334166\n",
            "02/19/2021 15:09:47 - INFO - processor.ner -   Writing example 130000 of 334166\n",
            "02/19/2021 15:09:49 - INFO - processor.ner -   Writing example 140000 of 334166\n",
            "02/19/2021 15:09:51 - INFO - processor.ner -   Writing example 150000 of 334166\n",
            "02/19/2021 15:09:54 - INFO - processor.ner -   Writing example 160000 of 334166\n",
            "02/19/2021 15:09:57 - INFO - processor.ner -   Writing example 170000 of 334166\n",
            "02/19/2021 15:09:59 - INFO - processor.ner -   Writing example 180000 of 334166\n",
            "02/19/2021 15:10:01 - INFO - processor.ner -   Writing example 190000 of 334166\n",
            "02/19/2021 15:10:03 - INFO - processor.ner -   Writing example 200000 of 334166\n",
            "02/19/2021 15:10:06 - INFO - processor.ner -   Writing example 210000 of 334166\n",
            "02/19/2021 15:10:08 - INFO - processor.ner -   Writing example 220000 of 334166\n",
            "02/19/2021 15:10:14 - INFO - processor.ner -   Writing example 230000 of 334166\n",
            "02/19/2021 15:10:20 - INFO - processor.ner -   Writing example 240000 of 334166\n",
            "02/19/2021 15:10:27 - INFO - processor.ner -   Writing example 250000 of 334166\n",
            "02/19/2021 15:10:35 - INFO - processor.ner -   Writing example 260000 of 334166\n",
            "02/19/2021 15:10:41 - INFO - processor.ner -   Writing example 270000 of 334166\n",
            "02/19/2021 15:10:48 - INFO - processor.ner -   Writing example 280000 of 334166\n",
            "02/19/2021 15:10:54 - INFO - processor.ner -   Writing example 290000 of 334166\n",
            "02/19/2021 15:11:01 - INFO - processor.ner -   Writing example 300000 of 334166\n",
            "02/19/2021 15:11:07 - INFO - processor.ner -   Writing example 310000 of 334166\n",
            "02/19/2021 15:11:14 - INFO - processor.ner -   Writing example 320000 of 334166\n",
            "02/19/2021 15:11:20 - INFO - processor.ner -   Writing example 330000 of 334166\n",
            "02/19/2021 15:11:23 - INFO - processor.ner -   Saving features into cached file data/cached_custom-ner_koelectra-small-discriminator_128_train\n",
            "02/19/2021 15:12:48 - INFO - processor.ner -   Creating features from dataset file at data\n",
            "02/19/2021 15:12:48 - INFO - processor.ner -   LOOKING AT data/custom-ner/dev.tsv\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   대내외 악재가 산적한 상황에서 저성장 기조가 굳어지고 있는 것이다.\tO O O O O O O O O\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   ◆주상복합 아파트에 해외 유명 건축가 참여 많아\tO O O O O O O\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   1부 리그보다 절실하다, 치열한 승격 경쟁\tQT-B O O O O O\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   정부의 3D TV 육성 정책과 3D TV 출시 등도 호재로 작용했다.\tOG-B O O O O O O O O O O\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   Writing example 0 of 37405\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   *** Example ***\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   guid: test-0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   tokens: [CLS] 대내 ##외 악 ##재가 산 ##적한 상황에서 저 ##성장 기조 ##가 굳 ##어지고 있는 것이다 . [SEP]\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   input_ids: 2 28403 6104 3361 15997 2795 20758 12005 3874 13537 22241 5692 380 18276 10552 10649 18 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   label: -100 12 -100 12 -100 12 -100 12 12 -100 12 -100 12 -100 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   *** Example ***\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   guid: test-1\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   tokens: [CLS] 2일 한국은행 ##에 따르면 3분기 실질 G ##N ##I는 전 분기 ##보다 0 . 4 % 감소했다 . [SEP]\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   input_ids: 2 12358 22027 5664 10730 17470 13224 43 5942 28476 3878 18509 10625 20 18 24 9 17664 18 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   label: -100 0 4 -100 12 0 12 12 -100 -100 12 12 -100 8 -100 -100 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   *** Example ***\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   guid: test-2\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   tokens: [CLS] 올 2분기 ( [UNK] 0 . 4 % ) 에 이어 2개 분기 연속 감소 ##세를 보였다 . [SEP]\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   input_ids: 2 3542 17686 12 1 20 18 24 9 13 3468 10659 14334 18509 12125 11473 11169 12285 18 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   label: -100 0 1 -100 -100 -100 -100 -100 -100 -100 -100 12 0 1 12 12 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   *** Example ***\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   guid: test-3\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   tokens: [CLS] 국제 ##유가 상승 ##과 수출 부진 ##으로 교역 조건이 나빠 ##진 결과로 풀이된다 . [SEP]\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   input_ids: 2 10917 14069 11211 5745 11611 14458 10540 23944 22180 19863 5679 20631 16299 18 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   label: -100 12 -100 12 -100 12 12 -100 12 12 12 -100 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   *** Example ***\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   guid: test-4\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   tokens: [CLS] 실질 G ##N ##I는 국민이 국내외 ##에서 벌어 ##들인 소득 ##의 총 ##합 ##에 교역 조건 변화를 반영한 것으로 국민의 실질 구매 ##력을 보여준다 . [SEP]\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   input_ids: 2 13224 43 5942 28476 15225 16589 10541 11874 17581 12142 5829 4489 5712 5664 23944 12557 14352 23665 10570 11604 13224 11892 10703 15433 18 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/19/2021 15:12:49 - INFO - processor.ner -   label: -100 12 12 -100 -100 12 12 -100 12 -100 12 -100 12 -100 -100 12 12 12 12 12 12 12 12 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/19/2021 15:12:55 - INFO - processor.ner -   Writing example 10000 of 37405\n",
            "02/19/2021 15:13:02 - INFO - processor.ner -   Writing example 20000 of 37405\n",
            "02/19/2021 15:13:08 - INFO - processor.ner -   Writing example 30000 of 37405\n",
            "02/19/2021 15:13:13 - INFO - processor.ner -   Saving features into cached file data/cached_custom-ner_koelectra-small-discriminator_128_test\n",
            "02/19/2021 15:13:20 - INFO - __main__ -   ***** Running training *****\n",
            "02/19/2021 15:13:20 - INFO - __main__ -     Num examples = 334166\n",
            "02/19/2021 15:13:20 - INFO - __main__ -     Num Epochs = 1\n",
            "02/19/2021 15:13:20 - INFO - __main__ -     Total train batch size = 64\n",
            "02/19/2021 15:13:20 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "02/19/2021 15:13:20 - INFO - __main__ -     Total optimization steps = 5222\n",
            "02/19/2021 15:13:20 - INFO - __main__ -     Logging steps = 1000\n",
            "02/19/2021 15:13:20 - INFO - __main__ -     Save steps = 1000\n",
            "02/19/2021 15:21:53 - INFO - __main__ -   ***** Running evaluation on test dataset (1000 step) *****\n",
            "02/19/2021 15:21:53 - INFO - __main__ -     Num examples = 37405\n",
            "02/19/2021 15:21:53 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/19/2021 15:23:42 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/19/2021 15:23:42 - INFO - __main__ -     f1 = 0.8664641462231638\n",
            "02/19/2021 15:23:42 - INFO - __main__ -     loss = 0.12795133325477917\n",
            "02/19/2021 15:23:42 - INFO - __main__ -     precision = 0.8466028135288836\n",
            "02/19/2021 15:23:42 - INFO - __main__ -     recall = 0.8872797615935588\n",
            "02/19/2021 15:23:51 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.90      0.94      0.92     13797\n",
            "          LC       0.85      0.80      0.82     12160\n",
            "          OG       0.78      0.85      0.81     18457\n",
            "          PS       0.84      0.93      0.88     14654\n",
            "          QT       0.89      0.93      0.91     16084\n",
            "          TI       0.79      0.78      0.78      1356\n",
            "\n",
            "   micro avg       0.85      0.89      0.87     76508\n",
            "   macro avg       0.84      0.87      0.85     76508\n",
            "weighted avg       0.85      0.89      0.87     76508\n",
            "\n",
            "02/19/2021 15:23:59 - INFO - __main__ -   Saving model checkpoint to ckpt/koelectra-small-custom-ner-ckpt/checkpoint-1000\n",
            "02/19/2021 15:32:30 - INFO - __main__ -   ***** Running evaluation on test dataset (2000 step) *****\n",
            "02/19/2021 15:32:30 - INFO - __main__ -     Num examples = 37405\n",
            "02/19/2021 15:32:30 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/19/2021 15:34:19 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/19/2021 15:34:19 - INFO - __main__ -     f1 = 0.8918280104228385\n",
            "02/19/2021 15:34:19 - INFO - __main__ -     loss = 0.09395725829304281\n",
            "02/19/2021 15:34:19 - INFO - __main__ -     precision = 0.8699521471630104\n",
            "02/19/2021 15:34:19 - INFO - __main__ -     recall = 0.9148324358237047\n",
            "02/19/2021 15:34:27 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.93      0.96      0.94     13797\n",
            "          LC       0.82      0.87      0.84     12160\n",
            "          OG       0.79      0.87      0.83     18457\n",
            "          PS       0.91      0.94      0.93     14654\n",
            "          QT       0.92      0.94      0.93     16084\n",
            "          TI       0.84      0.91      0.87      1356\n",
            "\n",
            "   micro avg       0.87      0.91      0.89     76508\n",
            "   macro avg       0.87      0.92      0.89     76508\n",
            "weighted avg       0.87      0.91      0.89     76508\n",
            "\n",
            "02/19/2021 15:34:36 - INFO - __main__ -   Saving model checkpoint to ckpt/koelectra-small-custom-ner-ckpt/checkpoint-2000\n",
            "02/19/2021 15:43:07 - INFO - __main__ -   ***** Running evaluation on test dataset (3000 step) *****\n",
            "02/19/2021 15:43:07 - INFO - __main__ -     Num examples = 37405\n",
            "02/19/2021 15:43:07 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/19/2021 15:44:57 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/19/2021 15:44:57 - INFO - __main__ -     f1 = 0.9060246253575325\n",
            "02/19/2021 15:44:57 - INFO - __main__ -     loss = 0.07927843509400867\n",
            "02/19/2021 15:44:57 - INFO - __main__ -     precision = 0.8952445357457287\n",
            "02/19/2021 15:44:57 - INFO - __main__ -     recall = 0.9170674962095468\n",
            "02/19/2021 15:45:05 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.95      0.96      0.96     13797\n",
            "          LC       0.86      0.86      0.86     12160\n",
            "          OG       0.85      0.86      0.86     18457\n",
            "          PS       0.89      0.96      0.92     14654\n",
            "          QT       0.94      0.95      0.94     16084\n",
            "          TI       0.87      0.92      0.89      1356\n",
            "\n",
            "   micro avg       0.90      0.92      0.91     76508\n",
            "   macro avg       0.89      0.92      0.90     76508\n",
            "weighted avg       0.90      0.92      0.91     76508\n",
            "\n",
            "02/19/2021 15:45:14 - INFO - __main__ -   Saving model checkpoint to ckpt/koelectra-small-custom-ner-ckpt/checkpoint-3000\n",
            "02/19/2021 15:53:44 - INFO - __main__ -   ***** Running evaluation on test dataset (4000 step) *****\n",
            "02/19/2021 15:53:44 - INFO - __main__ -     Num examples = 37405\n",
            "02/19/2021 15:53:44 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/19/2021 15:55:33 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/19/2021 15:55:33 - INFO - __main__ -     f1 = 0.9153505549450907\n",
            "02/19/2021 15:55:33 - INFO - __main__ -     loss = 0.0718919874547183\n",
            "02/19/2021 15:55:33 - INFO - __main__ -     precision = 0.9107140546764824\n",
            "02/19/2021 15:55:33 - INFO - __main__ -     recall = 0.9200345061954306\n",
            "02/19/2021 15:55:42 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.96      0.96      0.96     13797\n",
            "          LC       0.86      0.89      0.87     12160\n",
            "          OG       0.87      0.86      0.87     18457\n",
            "          PS       0.93      0.94      0.94     14654\n",
            "          QT       0.94      0.95      0.95     16084\n",
            "          TI       0.89      0.93      0.91      1356\n",
            "\n",
            "   micro avg       0.91      0.92      0.92     76508\n",
            "   macro avg       0.91      0.92      0.91     76508\n",
            "weighted avg       0.91      0.92      0.92     76508\n",
            "\n",
            "02/19/2021 15:55:50 - INFO - __main__ -   Saving model checkpoint to ckpt/koelectra-small-custom-ner-ckpt/checkpoint-4000\n",
            "02/19/2021 16:04:21 - INFO - __main__ -   ***** Running evaluation on test dataset (5000 step) *****\n",
            "02/19/2021 16:04:21 - INFO - __main__ -     Num examples = 37405\n",
            "02/19/2021 16:04:21 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/19/2021 16:06:10 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/19/2021 16:06:10 - INFO - __main__ -     f1 = 0.9151548113903889\n",
            "02/19/2021 16:06:10 - INFO - __main__ -     loss = 0.07077021659717421\n",
            "02/19/2021 16:06:10 - INFO - __main__ -     precision = 0.9067592188662783\n",
            "02/19/2021 16:06:10 - INFO - __main__ -     recall = 0.9237073247242118\n",
            "02/19/2021 16:06:19 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.95      0.96      0.96     13797\n",
            "          LC       0.88      0.87      0.87     12160\n",
            "          OG       0.85      0.88      0.87     18457\n",
            "          PS       0.93      0.95      0.94     14654\n",
            "          QT       0.94      0.95      0.95     16084\n",
            "          TI       0.88      0.93      0.90      1356\n",
            "\n",
            "   micro avg       0.91      0.92      0.92     76508\n",
            "   macro avg       0.90      0.92      0.91     76508\n",
            "weighted avg       0.91      0.92      0.92     76508\n",
            "\n",
            "02/19/2021 16:06:27 - INFO - __main__ -   Saving model checkpoint to ckpt/koelectra-small-custom-ner-ckpt/checkpoint-5000\n",
            "Epoch 1 done\n",
            "02/19/2021 16:08:21 - INFO - __main__ -    global_step = 5222, average loss = 0.11320822548345216\n",
            "02/19/2021 16:08:21 - INFO - __main__ -   Evaluate the following checkpoints: ['ckpt/koelectra-small-custom-ner-ckpt/checkpoint-1000', 'ckpt/koelectra-small-custom-ner-ckpt/checkpoint-2000', 'ckpt/koelectra-small-custom-ner-ckpt/checkpoint-3000', 'ckpt/koelectra-small-custom-ner-ckpt/checkpoint-4000', 'ckpt/koelectra-small-custom-ner-ckpt/checkpoint-5000']\n",
            "02/19/2021 16:08:22 - INFO - __main__ -   ***** Running evaluation on test dataset (1000 step) *****\n",
            "02/19/2021 16:08:22 - INFO - __main__ -     Num examples = 37405\n",
            "02/19/2021 16:08:22 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/19/2021 16:10:11 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/19/2021 16:10:11 - INFO - __main__ -     f1 = 0.8664641462231638\n",
            "02/19/2021 16:10:11 - INFO - __main__ -     loss = 0.12795133325477917\n",
            "02/19/2021 16:10:11 - INFO - __main__ -     precision = 0.8466028135288836\n",
            "02/19/2021 16:10:11 - INFO - __main__ -     recall = 0.8872797615935588\n",
            "02/19/2021 16:10:19 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.90      0.94      0.92     13797\n",
            "          LC       0.85      0.80      0.82     12160\n",
            "          OG       0.78      0.85      0.81     18457\n",
            "          PS       0.84      0.93      0.88     14654\n",
            "          QT       0.89      0.93      0.91     16084\n",
            "          TI       0.79      0.78      0.78      1356\n",
            "\n",
            "   micro avg       0.85      0.89      0.87     76508\n",
            "   macro avg       0.84      0.87      0.85     76508\n",
            "weighted avg       0.85      0.89      0.87     76508\n",
            "\n",
            "02/19/2021 16:10:28 - INFO - __main__ -   ***** Running evaluation on test dataset (2000 step) *****\n",
            "02/19/2021 16:10:28 - INFO - __main__ -     Num examples = 37405\n",
            "02/19/2021 16:10:28 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/19/2021 16:12:18 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/19/2021 16:12:18 - INFO - __main__ -     f1 = 0.8918280104228385\n",
            "02/19/2021 16:12:18 - INFO - __main__ -     loss = 0.09395725829304281\n",
            "02/19/2021 16:12:18 - INFO - __main__ -     precision = 0.8699521471630104\n",
            "02/19/2021 16:12:18 - INFO - __main__ -     recall = 0.9148324358237047\n",
            "02/19/2021 16:12:26 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.93      0.96      0.94     13797\n",
            "          LC       0.82      0.87      0.84     12160\n",
            "          OG       0.79      0.87      0.83     18457\n",
            "          PS       0.91      0.94      0.93     14654\n",
            "          QT       0.92      0.94      0.93     16084\n",
            "          TI       0.84      0.91      0.87      1356\n",
            "\n",
            "   micro avg       0.87      0.91      0.89     76508\n",
            "   macro avg       0.87      0.92      0.89     76508\n",
            "weighted avg       0.87      0.91      0.89     76508\n",
            "\n",
            "02/19/2021 16:12:35 - INFO - __main__ -   ***** Running evaluation on test dataset (3000 step) *****\n",
            "02/19/2021 16:12:35 - INFO - __main__ -     Num examples = 37405\n",
            "02/19/2021 16:12:35 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/19/2021 16:14:25 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/19/2021 16:14:25 - INFO - __main__ -     f1 = 0.9060246253575325\n",
            "02/19/2021 16:14:25 - INFO - __main__ -     loss = 0.07927843509400867\n",
            "02/19/2021 16:14:25 - INFO - __main__ -     precision = 0.8952445357457287\n",
            "02/19/2021 16:14:25 - INFO - __main__ -     recall = 0.9170674962095468\n",
            "02/19/2021 16:14:33 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.95      0.96      0.96     13797\n",
            "          LC       0.86      0.86      0.86     12160\n",
            "          OG       0.85      0.86      0.86     18457\n",
            "          PS       0.89      0.96      0.92     14654\n",
            "          QT       0.94      0.95      0.94     16084\n",
            "          TI       0.87      0.92      0.89      1356\n",
            "\n",
            "   micro avg       0.90      0.92      0.91     76508\n",
            "   macro avg       0.89      0.92      0.90     76508\n",
            "weighted avg       0.90      0.92      0.91     76508\n",
            "\n",
            "02/19/2021 16:14:43 - INFO - __main__ -   ***** Running evaluation on test dataset (4000 step) *****\n",
            "02/19/2021 16:14:43 - INFO - __main__ -     Num examples = 37405\n",
            "02/19/2021 16:14:43 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/19/2021 16:16:32 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/19/2021 16:16:32 - INFO - __main__ -     f1 = 0.9153505549450907\n",
            "02/19/2021 16:16:32 - INFO - __main__ -     loss = 0.0718919874547183\n",
            "02/19/2021 16:16:32 - INFO - __main__ -     precision = 0.9107140546764824\n",
            "02/19/2021 16:16:32 - INFO - __main__ -     recall = 0.9200345061954306\n",
            "02/19/2021 16:16:41 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.96      0.96      0.96     13797\n",
            "          LC       0.86      0.89      0.87     12160\n",
            "          OG       0.87      0.86      0.87     18457\n",
            "          PS       0.93      0.94      0.94     14654\n",
            "          QT       0.94      0.95      0.95     16084\n",
            "          TI       0.89      0.93      0.91      1356\n",
            "\n",
            "   micro avg       0.91      0.92      0.92     76508\n",
            "   macro avg       0.91      0.92      0.91     76508\n",
            "weighted avg       0.91      0.92      0.92     76508\n",
            "\n",
            "02/19/2021 16:16:50 - INFO - __main__ -   ***** Running evaluation on test dataset (5000 step) *****\n",
            "02/19/2021 16:16:50 - INFO - __main__ -     Num examples = 37405\n",
            "02/19/2021 16:16:50 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/19/2021 16:18:39 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/19/2021 16:18:39 - INFO - __main__ -     f1 = 0.9151548113903889\n",
            "02/19/2021 16:18:39 - INFO - __main__ -     loss = 0.07077021659717421\n",
            "02/19/2021 16:18:39 - INFO - __main__ -     precision = 0.9067592188662783\n",
            "02/19/2021 16:18:39 - INFO - __main__ -     recall = 0.9237073247242118\n",
            "02/19/2021 16:18:48 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.95      0.96      0.96     13797\n",
            "          LC       0.88      0.87      0.87     12160\n",
            "          OG       0.85      0.88      0.87     18457\n",
            "          PS       0.93      0.95      0.94     14654\n",
            "          QT       0.94      0.95      0.95     16084\n",
            "          TI       0.88      0.93      0.90      1356\n",
            "\n",
            "   micro avg       0.91      0.92      0.92     76508\n",
            "   macro avg       0.90      0.92      0.91     76508\n",
            "weighted avg       0.91      0.92      0.92     76508\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-Ak92ESkUOv",
        "outputId": "16ef9771-97a8-4be6-e9f5-b6b4ed526f6b"
      },
      "source": [
        "!python3 run_ner.py --task custom-ner --config_file kobert.json # config/custom-ner 에서 원하는 모델의 config를 변경하고 넣어주세요."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-21 07:50:23.434392: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/21/2021 07:50:28 - INFO - filelock -   Lock 139902614124528 acquired on /root/.cache/torch/transformers/062a4aa346f4830513250d6b61df96193c3781f3c6abdf2f78d619dc2fb82a8b.f333d098350ff3fff2438ea37aa328f28d79a0a9b992aa89e9ba3e1184094cb0.lock\n",
            "Downloading: 100% 426/426 [00:00<00:00, 705kB/s]\n",
            "02/21/2021 07:50:28 - INFO - filelock -   Lock 139902614124528 released on /root/.cache/torch/transformers/062a4aa346f4830513250d6b61df96193c3781f3c6abdf2f78d619dc2fb82a8b.f333d098350ff3fff2438ea37aa328f28d79a0a9b992aa89e9ba3e1184094cb0.lock\n",
            "02/21/2021 07:50:28 - INFO - filelock -   Lock 139901367448464 acquired on /root/.cache/torch/transformers/1f871cf1d80a4c09742b5f2094cc7ab9163dfbcfb930d99fdc9149aebf6a13bb.7555cd8e0e341a4d19f33dcb20eaf8614c72f56827d1b28481046caabba18eb3.lock\n",
            "Downloading: 100% 371k/371k [00:00<00:00, 21.8MB/s]\n",
            "02/21/2021 07:50:28 - INFO - filelock -   Lock 139901367448464 released on /root/.cache/torch/transformers/1f871cf1d80a4c09742b5f2094cc7ab9163dfbcfb930d99fdc9149aebf6a13bb.7555cd8e0e341a4d19f33dcb20eaf8614c72f56827d1b28481046caabba18eb3.lock\n",
            "02/21/2021 07:50:28 - INFO - filelock -   Lock 139901367451600 acquired on /root/.cache/torch/transformers/c247d0225676d99ae7f3e6d39dba68b77756181415828f922c6393e4aecb53ce.5d8505da4be274704344000e8d9a798ff03b513f5053f98f7e97d719a9a9891b.lock\n",
            "Downloading: 100% 77.8k/77.8k [00:00<00:00, 14.4MB/s]\n",
            "02/21/2021 07:50:28 - INFO - filelock -   Lock 139901367451600 released on /root/.cache/torch/transformers/c247d0225676d99ae7f3e6d39dba68b77756181415828f922c6393e4aecb53ce.5d8505da4be274704344000e8d9a798ff03b513f5053f98f7e97d719a9a9891b.lock\n",
            "02/21/2021 07:50:28 - INFO - filelock -   Lock 139901367451376 acquired on /root/.cache/torch/transformers/978cb38aad4ee488a828e14ba50bbbd123e0349dd36888c699863ed536f0df92.03d813488ed0975c964f8c43b92c0aad2c8d7455b37e76c78509912083a3982c.lock\n",
            "Downloading: 100% 369M/369M [00:04<00:00, 90.5MB/s]\n",
            "02/21/2021 07:50:32 - INFO - filelock -   Lock 139901367451376 released on /root/.cache/torch/transformers/978cb38aad4ee488a828e14ba50bbbd123e0349dd36888c699863ed536f0df92.03d813488ed0975c964f8c43b92c0aad2c8d7455b37e76c78509912083a3982c.lock\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "02/21/2021 07:50:49 - INFO - processor.ner -   Creating features from dataset file at data\n",
            "02/21/2021 07:50:49 - INFO - processor.ner -   LOOKING AT data/custom-ner/train.tsv\n",
            "02/21/2021 07:50:49 - INFO - processor.ner -   네 여러분 안녕하십니까. 엄길청입니다.\tO O O PS-B\n",
            "02/21/2021 07:50:49 - INFO - processor.ner -   국립 공원을 여기저기 그냥 지정하는 것으로 하지 말고\tO O O O O O O O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   근데 저 개인적으로는 땅콩소스를 싫어해요.\tO O O O O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   그 공간이 따로 있을 정도라고 해요.\tO O O O O O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   이 교과서를 정부가 포기하지 않는다면\tO O OG-B O O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   또 뭔가 예의 바르고 어~ 정말 별 많은 나라로\tO O O O O O O O O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   놨겠네 일주일 계시다가 나왔으니\tO DT-B O O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   다운그레이드 되셨습니다\tO O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   아니 근데도 굉장히 말라 말랐는데\tO O O O O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   마이크에 대고.\tO O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   사회에 던지신 바가 있습니다만\tO O O O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   네.\tO\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   불만 부분도 어~\tO O O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   그래서 그냥\tO O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   지금 전화를 해서 지금이라도 전화 근데 전화를 하면 안 받을 것 같았고 이미 늦었고\tO O O O O O O O O O O O O O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   네 또 질문하실 분 계신가요.\tO O O O O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   네. 이제 마늘 다짐 너 줘야지 맛있겠죠?\tO O O O O O O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   뭐라도 좀 힐링을 해야겠다 자체적으로\tO O O O O\n",
            "02/21/2021 07:50:50 - INFO - processor.ner -   박근혜 영애한테\tPS-B O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   네.\tO\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   아몬드 넣고 뭐\tO O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   한국문학연구가 문학의 에 본래로 돌아가기 위해선\tO O O O O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   아주 철저하게 하고 있습니다 그래서\tO O O O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   카타르월드컵 개최 시기는 새달 19일 스위스 취리히에서 열리는 피파 집행위원회에서 최종 확정된다.\tO O O DT-B DT-I LC-B LC-B O OG-B O O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   중국은 이번 조처를 통해서 미국 등이 요구하던 시장에 환율을 맡기는 변동환율제로 한걸음 더 나아가는 명분을 얻었다.\tOG-B O O O OG-B O O O O O O QT-B O O O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   일반학기(필수)에 방학기간인 6주 동안 집중학기(선택)를 더해 학점을 취득하는 방식으로 조기졸업이 가능하다.\tO O DT-B DT-I O O O O O O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   4월 장익제를 시작으로 이경훈 김형성 김경태 류현우 등이 차례로 우승컵을 들어올렸다.\tDT-B PS-B O PS-B PS-B PS-B PS-B O O O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   100㎡(30평) 남짓한 실내에 26명의 수강생이 빼곡히 들어앉아 진지한 표정으로 강사의 설명에 귀를 기울였다.\tQT-B O O QT-B O O O O O O O O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   반면 고용조정이 불가피한 사업주가 휴업·훈련 등을 통해 노동자의 고용을 유지하면 지급하는 고용유지지원금 등 고용 관련 다른 지표는 호전된 것으로 나타났다.\tO O O O O O O O O O O O O O O O O O O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   동등 지분 확보 ‘기술먹튀’ 방지 의도\tO O O O O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   '스두'와 '쿵차오'란 뭘까.\tO O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   1943년 체코 법원의 궐석재판에서 사형이 선고된 그는 캐나다로 도주해 수십년간 가명으로 예술품 거래상을 하다 1995년 신분이 탄로나 다시 도망자 신세가 됐다.\tDT-B OG-B OG-I O O O O LC-B O O O O O O DT-B O O O O O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   회사와 쌍용차지부는 1시간가량 이어진 교섭이 끝난 뒤 정리해고 사태를 해결하기 위한 “4대 의제를 확정하고 실무교섭을 빠르게 진행하기로 합의했다”고 밝혔다.\tO OG-B TI-B O O O O O O O O O O O O O O O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   범정부사고대책본부 관계자는 25일 <한겨레>와 한 통화에서 “언딘의 활동이 선박 인양이 주목적인 것은 맞다.\tOG-B O DT-B O O O O O O O O O O\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   Writing example 0 of 334166\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   *** Example ***\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   guid: train-0\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   tokens: [CLS] ▁네 ▁여러분 ▁안 녕 하 십 니까 . ▁엄 길 청 입니다 . [SEP]\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   input_ids: 2 1469 3305 3135 5724 7782 6749 5771 54 3255 5585 7431 7139 54 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   label: -100 12 12 12 -100 -100 -100 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   *** Example ***\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   guid: train-1\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   tokens: [CLS] ▁마 냥 ▁행복 하고 ▁사랑 만 ▁느끼 고 ▁싶은 ▁계절 ▁오 월 입니다 . [SEP]\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   input_ids: 2 1907 5690 5025 7788 2590 6150 1546 5439 3075 988 3417 7028 7139 54 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   label: -100 12 -100 12 -100 12 -100 12 -100 12 12 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   *** Example ***\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   guid: train-2\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   tokens: [CLS] ▁가정 의 ▁달 도 ▁오 월 이 죠 . ▁사 소 한 ▁말 ▁끝에 ▁큰 ▁ 싸움 이 ▁나 기도 ▁하 구 요 . [SEP]\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   input_ids: 2 762 7095 1597 5859 3417 7028 7096 7275 54 2573 6607 7828 1958 1368 4688 517 6753 7096 1370 5570 4924 5495 6999 54 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   label: -100 12 -100 12 -100 0 -100 -100 -100 -100 12 -100 -100 12 12 12 12 -100 -100 12 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   *** Example ***\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   guid: train-3\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   tokens: [CLS] ▁또 ▁법 보다 ▁주 먹 이 ▁ 가 깝 다 는 ▁생각 도 ▁ 가 끔 ▁들 ▁때 가 ▁있습니다 . [SEP]\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   input_ids: 2 1861 2322 6371 4213 6183 7096 517 5330 5599 5782 5760 2705 5859 517 5330 5646 1801 1844 5330 3867 54 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   label: -100 12 12 -100 12 -100 -100 12 -100 -100 -100 -100 12 -100 12 -100 -100 12 12 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   *** Example ***\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   guid: train-4\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   tokens: [CLS] ▁학교폭력 도 ▁따 지고 ▁보면 ▁말 ▁때문에 ▁일어나 는 ▁일이 ▁많지 ▁않을까 ▁ 싶 습니다 . [SEP]\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   input_ids: 2 4950 5859 1833 7321 2372 1958 1849 3813 5760 3818 1957 3164 517 6751 6701 54 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:50:51 - INFO - processor.ner -   label: -100 12 -100 12 -100 12 12 12 12 -100 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/21/2021 07:50:53 - INFO - processor.ner -   Writing example 10000 of 334166\n",
            "02/21/2021 07:50:54 - INFO - processor.ner -   Writing example 20000 of 334166\n",
            "02/21/2021 07:50:55 - INFO - processor.ner -   Writing example 30000 of 334166\n",
            "02/21/2021 07:50:56 - INFO - processor.ner -   Writing example 40000 of 334166\n",
            "02/21/2021 07:50:58 - INFO - processor.ner -   Writing example 50000 of 334166\n",
            "02/21/2021 07:50:59 - INFO - processor.ner -   Writing example 60000 of 334166\n",
            "02/21/2021 07:51:01 - INFO - processor.ner -   Writing example 70000 of 334166\n",
            "02/21/2021 07:51:02 - INFO - processor.ner -   Writing example 80000 of 334166\n",
            "02/21/2021 07:51:04 - INFO - processor.ner -   Writing example 90000 of 334166\n",
            "02/21/2021 07:51:05 - INFO - processor.ner -   Writing example 100000 of 334166\n",
            "02/21/2021 07:51:06 - INFO - processor.ner -   Writing example 110000 of 334166\n",
            "02/21/2021 07:51:08 - INFO - processor.ner -   Writing example 120000 of 334166\n",
            "02/21/2021 07:51:09 - INFO - processor.ner -   Writing example 130000 of 334166\n",
            "02/21/2021 07:51:10 - INFO - processor.ner -   Writing example 140000 of 334166\n",
            "02/21/2021 07:51:11 - INFO - processor.ner -   Writing example 150000 of 334166\n",
            "02/21/2021 07:51:13 - INFO - processor.ner -   Writing example 160000 of 334166\n",
            "02/21/2021 07:51:15 - INFO - processor.ner -   Writing example 170000 of 334166\n",
            "02/21/2021 07:51:16 - INFO - processor.ner -   Writing example 180000 of 334166\n",
            "02/21/2021 07:51:17 - INFO - processor.ner -   Writing example 190000 of 334166\n",
            "02/21/2021 07:51:18 - INFO - processor.ner -   Writing example 200000 of 334166\n",
            "02/21/2021 07:51:20 - INFO - processor.ner -   Writing example 210000 of 334166\n",
            "02/21/2021 07:51:21 - INFO - processor.ner -   Writing example 220000 of 334166\n",
            "02/21/2021 07:51:24 - INFO - processor.ner -   Writing example 230000 of 334166\n",
            "02/21/2021 07:51:27 - INFO - processor.ner -   Writing example 240000 of 334166\n",
            "02/21/2021 07:51:31 - INFO - processor.ner -   Writing example 250000 of 334166\n",
            "02/21/2021 07:51:34 - INFO - processor.ner -   Writing example 260000 of 334166\n",
            "02/21/2021 07:51:39 - INFO - processor.ner -   Writing example 270000 of 334166\n",
            "02/21/2021 07:51:42 - INFO - processor.ner -   Writing example 280000 of 334166\n",
            "02/21/2021 07:51:46 - INFO - processor.ner -   Writing example 290000 of 334166\n",
            "02/21/2021 07:51:49 - INFO - processor.ner -   Writing example 300000 of 334166\n",
            "02/21/2021 07:51:53 - INFO - processor.ner -   Writing example 310000 of 334166\n",
            "02/21/2021 07:51:56 - INFO - processor.ner -   Writing example 320000 of 334166\n",
            "02/21/2021 07:51:59 - INFO - processor.ner -   Writing example 330000 of 334166\n",
            "02/21/2021 07:52:01 - INFO - processor.ner -   Saving features into cached file data/cached_custom-ner_kobert_128_train\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   Creating features from dataset file at data\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   LOOKING AT data/custom-ner/dev.tsv\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   대내외 악재가 산적한 상황에서 저성장 기조가 굳어지고 있는 것이다.\tO O O O O O O O O\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   ◆주상복합 아파트에 해외 유명 건축가 참여 많아\tO O O O O O O\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   1부 리그보다 절실하다, 치열한 승격 경쟁\tQT-B O O O O O\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   정부의 3D TV 육성 정책과 3D TV 출시 등도 호재로 작용했다.\tOG-B O O O O O O O O O O\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   Writing example 0 of 37405\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   *** Example ***\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   guid: test-0\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   tokens: [CLS] ▁대 내 외 ▁악 재 가 ▁산 적 한 ▁상황에서 ▁저 성장 ▁기조 가 ▁ 굳 어 지고 ▁있는 ▁것이다 . [SEP]\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   input_ids: 2 1633 5678 6995 3133 7191 5330 2640 7202 7828 2691 3990 6578 1299 5330 517 5515 6855 7321 3860 913 54 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   label: -100 12 -100 -100 12 -100 -100 12 -100 -100 12 12 -100 12 -100 12 -100 -100 -100 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   *** Example ***\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   guid: test-1\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   tokens: [CLS] ▁2 일 ▁한국은행 에 ▁따르면 ▁3 분기 ▁실 질 ▁G N I 는 ▁전 ▁ 분기 보다 ▁ 0.4% ▁감소했다 . [SEP]\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   input_ids: 2 553 7126 4961 6896 1838 589 6418 3036 7350 650 322 296 5760 4012 517 6418 6371 517 70 798 54 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   label: -100 0 -100 4 -100 12 0 -100 12 -100 12 -100 -100 -100 12 12 -100 -100 8 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   *** Example ***\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   guid: test-2\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   tokens: [CLS] ▁올 ▁2 분기 ( ― 0.4% ) 에 ▁이어 ▁2 개 ▁ 분기 ▁연속 ▁감소 세를 ▁보였다 . [SEP]\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   input_ids: 2 3439 553 6418 18 496 70 40 6896 3716 553 5357 517 6418 3349 796 6585 2381 54 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   label: -100 0 1 -100 -100 -100 -100 -100 -100 12 0 -100 1 -100 12 12 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   *** Example ***\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   guid: test-3\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   tokens: [CLS] ▁국제 유 가 ▁상승 과 ▁수출 ▁부진 으로 ▁교 역 ▁조건 이 ▁나 빠 진 ▁결과 로 ▁풀이된다 . [SEP]\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   input_ids: 2 1155 7063 5330 2670 5468 2906 2454 7078 1103 6926 4163 7096 1370 6459 7344 938 6079 4890 54 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   label: -100 12 -100 -100 12 -100 12 12 -100 12 -100 12 -100 12 -100 -100 12 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   *** Example ***\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   guid: test-4\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   tokens: [CLS] ▁실 질 ▁G N I 는 ▁국민 이 ▁국내외 에서 ▁벌 어 들 인 ▁소득 의 ▁총 합 에 ▁교 역 ▁조건 ▁변화를 ▁반영 한 ▁것으로 ▁국민의 ▁실 질 ▁구매 력을 ▁보여준 다 . [SEP]\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   input_ids: 2 3036 7350 650 322 296 5760 1144 7096 1140 6903 2309 6855 5931 7119 2828 7095 4512 7842 6896 1103 6926 4163 2347 2216 7828 909 1148 3036 7350 1119 6065 2378 5782 54 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/21/2021 07:53:04 - INFO - processor.ner -   label: -100 12 -100 12 -100 -100 -100 12 -100 12 -100 12 -100 -100 -100 12 -100 12 -100 -100 12 -100 12 12 12 -100 12 12 12 -100 12 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 \n",
            "02/21/2021 07:53:08 - INFO - processor.ner -   Writing example 10000 of 37405\n",
            "02/21/2021 07:53:12 - INFO - processor.ner -   Writing example 20000 of 37405\n",
            "02/21/2021 07:53:15 - INFO - processor.ner -   Writing example 30000 of 37405\n",
            "02/21/2021 07:53:17 - INFO - processor.ner -   Saving features into cached file data/cached_custom-ner_kobert_128_test\n",
            "02/21/2021 07:53:23 - INFO - __main__ -   ***** Running training *****\n",
            "02/21/2021 07:53:23 - INFO - __main__ -     Num examples = 334166\n",
            "02/21/2021 07:53:23 - INFO - __main__ -     Num Epochs = 1\n",
            "02/21/2021 07:53:23 - INFO - __main__ -     Total train batch size = 64\n",
            "02/21/2021 07:53:23 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "02/21/2021 07:53:23 - INFO - __main__ -     Total optimization steps = 5222\n",
            "02/21/2021 07:53:23 - INFO - __main__ -     Logging steps = 1000\n",
            "02/21/2021 07:53:23 - INFO - __main__ -     Save steps = 1000\n",
            "02/21/2021 08:15:48 - INFO - __main__ -   ***** Running evaluation on test dataset (1000 step) *****\n",
            "02/21/2021 08:15:48 - INFO - __main__ -     Num examples = 37405\n",
            "02/21/2021 08:15:48 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/21/2021 08:20:48 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/21/2021 08:20:48 - INFO - __main__ -     f1 = 0.9080211580722645\n",
            "02/21/2021 08:20:48 - INFO - __main__ -     loss = 0.07884196429783574\n",
            "02/21/2021 08:20:48 - INFO - __main__ -     precision = 0.8878741692069362\n",
            "02/21/2021 08:20:48 - INFO - __main__ -     recall = 0.9291036971186529\n",
            "02/21/2021 08:20:55 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.95      0.96      0.95     13795\n",
            "          LC       0.77      0.94      0.84     12157\n",
            "          OG       0.88      0.84      0.86     18454\n",
            "          PS       0.93      0.97      0.95     14649\n",
            "          QT       0.92      0.95      0.94     16082\n",
            "          TI       0.85      0.91      0.88      1355\n",
            "\n",
            "   micro avg       0.89      0.93      0.91     76492\n",
            "   macro avg       0.88      0.93      0.90     76492\n",
            "weighted avg       0.89      0.93      0.91     76492\n",
            "\n",
            "02/21/2021 08:21:04 - INFO - __main__ -   Saving model checkpoint to ckpt/kobert-custom-ner-ckpt/checkpoint-1000\n",
            "02/21/2021 08:43:31 - INFO - __main__ -   ***** Running evaluation on test dataset (2000 step) *****\n",
            "02/21/2021 08:43:31 - INFO - __main__ -     Num examples = 37405\n",
            "02/21/2021 08:43:31 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/21/2021 08:48:31 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/21/2021 08:48:31 - INFO - __main__ -     f1 = 0.9243876877692815\n",
            "02/21/2021 08:48:31 - INFO - __main__ -     loss = 0.06173394579732784\n",
            "02/21/2021 08:48:31 - INFO - __main__ -     precision = 0.9149527444100095\n",
            "02/21/2021 08:48:31 - INFO - __main__ -     recall = 0.9340192438424934\n",
            "02/21/2021 08:48:39 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.95      0.97      0.96     13795\n",
            "          LC       0.85      0.93      0.89     12157\n",
            "          OG       0.88      0.87      0.88     18454\n",
            "          PS       0.97      0.94      0.96     14649\n",
            "          QT       0.93      0.97      0.95     16082\n",
            "          TI       0.86      0.94      0.90      1355\n",
            "\n",
            "   micro avg       0.91      0.93      0.92     76492\n",
            "   macro avg       0.91      0.94      0.92     76492\n",
            "weighted avg       0.92      0.93      0.92     76492\n",
            "\n",
            "02/21/2021 08:48:48 - INFO - __main__ -   Saving model checkpoint to ckpt/kobert-custom-ner-ckpt/checkpoint-2000\n",
            "02/21/2021 09:11:15 - INFO - __main__ -   ***** Running evaluation on test dataset (3000 step) *****\n",
            "02/21/2021 09:11:15 - INFO - __main__ -     Num examples = 37405\n",
            "02/21/2021 09:11:15 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/21/2021 09:16:15 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/21/2021 09:16:15 - INFO - __main__ -     f1 = 0.9326816984046626\n",
            "02/21/2021 09:16:15 - INFO - __main__ -     loss = 0.05367720473570425\n",
            "02/21/2021 09:16:15 - INFO - __main__ -     precision = 0.9261297723698796\n",
            "02/21/2021 09:16:15 - INFO - __main__ -     recall = 0.9393269884432359\n",
            "02/21/2021 09:16:22 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.96      0.97      0.96     13795\n",
            "          LC       0.85      0.94      0.89     12157\n",
            "          OG       0.92      0.87      0.89     18454\n",
            "          PS       0.97      0.97      0.97     14649\n",
            "          QT       0.94      0.97      0.95     16082\n",
            "          TI       0.88      0.94      0.91      1355\n",
            "\n",
            "   micro avg       0.93      0.94      0.93     76492\n",
            "   macro avg       0.92      0.94      0.93     76492\n",
            "weighted avg       0.93      0.94      0.93     76492\n",
            "\n",
            "02/21/2021 09:16:31 - INFO - __main__ -   Saving model checkpoint to ckpt/kobert-custom-ner-ckpt/checkpoint-3000\n",
            "02/21/2021 09:38:57 - INFO - __main__ -   ***** Running evaluation on test dataset (4000 step) *****\n",
            "02/21/2021 09:38:57 - INFO - __main__ -     Num examples = 37405\n",
            "02/21/2021 09:38:57 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/21/2021 09:43:57 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/21/2021 09:43:57 - INFO - __main__ -     f1 = 0.9395683640190741\n",
            "02/21/2021 09:43:57 - INFO - __main__ -     loss = 0.04831104213848659\n",
            "02/21/2021 09:43:57 - INFO - __main__ -     precision = 0.9338533460753674\n",
            "02/21/2021 09:43:57 - INFO - __main__ -     recall = 0.9453537624849657\n",
            "02/21/2021 09:44:04 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.96      0.97      0.97     13795\n",
            "          LC       0.91      0.90      0.91     12157\n",
            "          OG       0.89      0.91      0.90     18454\n",
            "          PS       0.97      0.97      0.97     14649\n",
            "          QT       0.95      0.97      0.96     16082\n",
            "          TI       0.89      0.94      0.92      1355\n",
            "\n",
            "   micro avg       0.93      0.95      0.94     76492\n",
            "   macro avg       0.93      0.95      0.94     76492\n",
            "weighted avg       0.93      0.95      0.94     76492\n",
            "\n",
            "02/21/2021 09:44:13 - INFO - __main__ -   Saving model checkpoint to ckpt/kobert-custom-ner-ckpt/checkpoint-4000\n",
            "02/21/2021 10:06:39 - INFO - __main__ -   ***** Running evaluation on test dataset (5000 step) *****\n",
            "02/21/2021 10:06:39 - INFO - __main__ -     Num examples = 37405\n",
            "02/21/2021 10:06:39 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/21/2021 10:11:39 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/21/2021 10:11:39 - INFO - __main__ -     f1 = 0.9415486748612624\n",
            "02/21/2021 10:11:39 - INFO - __main__ -     loss = 0.046225458224631415\n",
            "02/21/2021 10:11:39 - INFO - __main__ -     precision = 0.9338957765317146\n",
            "02/21/2021 10:11:39 - INFO - __main__ -     recall = 0.949328034304241\n",
            "02/21/2021 10:11:46 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.96      0.98      0.97     13795\n",
            "          LC       0.91      0.92      0.91     12157\n",
            "          OG       0.89      0.91      0.90     18454\n",
            "          PS       0.97      0.98      0.97     14649\n",
            "          QT       0.95      0.97      0.96     16082\n",
            "          TI       0.89      0.95      0.92      1355\n",
            "\n",
            "   micro avg       0.93      0.95      0.94     76492\n",
            "   macro avg       0.93      0.95      0.94     76492\n",
            "weighted avg       0.93      0.95      0.94     76492\n",
            "\n",
            "02/21/2021 10:11:55 - INFO - __main__ -   Saving model checkpoint to ckpt/kobert-custom-ner-ckpt/checkpoint-5000\n",
            "Epoch 1 done\n",
            "02/21/2021 10:16:54 - INFO - __main__ -    global_step = 5222, average loss = 0.0776301990913235\n",
            "02/21/2021 10:16:54 - INFO - __main__ -   Evaluate the following checkpoints: ['ckpt/kobert-custom-ner-ckpt/checkpoint-1000', 'ckpt/kobert-custom-ner-ckpt/checkpoint-2000', 'ckpt/kobert-custom-ner-ckpt/checkpoint-3000', 'ckpt/kobert-custom-ner-ckpt/checkpoint-4000', 'ckpt/kobert-custom-ner-ckpt/checkpoint-5000']\n",
            "02/21/2021 10:17:06 - INFO - __main__ -   ***** Running evaluation on test dataset (1000 step) *****\n",
            "02/21/2021 10:17:06 - INFO - __main__ -     Num examples = 37405\n",
            "02/21/2021 10:17:06 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/21/2021 10:22:06 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/21/2021 10:22:06 - INFO - __main__ -     f1 = 0.9080211580722645\n",
            "02/21/2021 10:22:06 - INFO - __main__ -     loss = 0.07884196429783574\n",
            "02/21/2021 10:22:06 - INFO - __main__ -     precision = 0.8878741692069362\n",
            "02/21/2021 10:22:06 - INFO - __main__ -     recall = 0.9291036971186529\n",
            "02/21/2021 10:22:13 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.95      0.96      0.95     13795\n",
            "          LC       0.77      0.94      0.84     12157\n",
            "          OG       0.88      0.84      0.86     18454\n",
            "          PS       0.93      0.97      0.95     14649\n",
            "          QT       0.92      0.95      0.94     16082\n",
            "          TI       0.85      0.91      0.88      1355\n",
            "\n",
            "   micro avg       0.89      0.93      0.91     76492\n",
            "   macro avg       0.88      0.93      0.90     76492\n",
            "weighted avg       0.89      0.93      0.91     76492\n",
            "\n",
            "02/21/2021 10:22:31 - INFO - __main__ -   ***** Running evaluation on test dataset (2000 step) *****\n",
            "02/21/2021 10:22:31 - INFO - __main__ -     Num examples = 37405\n",
            "02/21/2021 10:22:31 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/21/2021 10:27:31 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/21/2021 10:27:31 - INFO - __main__ -     f1 = 0.9243876877692815\n",
            "02/21/2021 10:27:31 - INFO - __main__ -     loss = 0.06173394579732784\n",
            "02/21/2021 10:27:31 - INFO - __main__ -     precision = 0.9149527444100095\n",
            "02/21/2021 10:27:31 - INFO - __main__ -     recall = 0.9340192438424934\n",
            "02/21/2021 10:27:38 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.95      0.97      0.96     13795\n",
            "          LC       0.85      0.93      0.89     12157\n",
            "          OG       0.88      0.87      0.88     18454\n",
            "          PS       0.97      0.94      0.96     14649\n",
            "          QT       0.93      0.97      0.95     16082\n",
            "          TI       0.86      0.94      0.90      1355\n",
            "\n",
            "   micro avg       0.91      0.93      0.92     76492\n",
            "   macro avg       0.91      0.94      0.92     76492\n",
            "weighted avg       0.92      0.93      0.92     76492\n",
            "\n",
            "02/21/2021 10:27:48 - INFO - __main__ -   ***** Running evaluation on test dataset (3000 step) *****\n",
            "02/21/2021 10:27:48 - INFO - __main__ -     Num examples = 37405\n",
            "02/21/2021 10:27:48 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/21/2021 10:32:48 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/21/2021 10:32:48 - INFO - __main__ -     f1 = 0.9326816984046626\n",
            "02/21/2021 10:32:48 - INFO - __main__ -     loss = 0.05367720473570425\n",
            "02/21/2021 10:32:48 - INFO - __main__ -     precision = 0.9261297723698796\n",
            "02/21/2021 10:32:48 - INFO - __main__ -     recall = 0.9393269884432359\n",
            "02/21/2021 10:32:55 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.96      0.97      0.96     13795\n",
            "          LC       0.85      0.94      0.89     12157\n",
            "          OG       0.92      0.87      0.89     18454\n",
            "          PS       0.97      0.97      0.97     14649\n",
            "          QT       0.94      0.97      0.95     16082\n",
            "          TI       0.88      0.94      0.91      1355\n",
            "\n",
            "   micro avg       0.93      0.94      0.93     76492\n",
            "   macro avg       0.92      0.94      0.93     76492\n",
            "weighted avg       0.93      0.94      0.93     76492\n",
            "\n",
            "02/21/2021 10:33:05 - INFO - __main__ -   ***** Running evaluation on test dataset (4000 step) *****\n",
            "02/21/2021 10:33:05 - INFO - __main__ -     Num examples = 37405\n",
            "02/21/2021 10:33:05 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/21/2021 10:38:05 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/21/2021 10:38:05 - INFO - __main__ -     f1 = 0.9395683640190741\n",
            "02/21/2021 10:38:05 - INFO - __main__ -     loss = 0.04831104213848659\n",
            "02/21/2021 10:38:05 - INFO - __main__ -     precision = 0.9338533460753674\n",
            "02/21/2021 10:38:05 - INFO - __main__ -     recall = 0.9453537624849657\n",
            "02/21/2021 10:38:12 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.96      0.97      0.97     13795\n",
            "          LC       0.91      0.90      0.91     12157\n",
            "          OG       0.89      0.91      0.90     18454\n",
            "          PS       0.97      0.97      0.97     14649\n",
            "          QT       0.95      0.97      0.96     16082\n",
            "          TI       0.89      0.94      0.92      1355\n",
            "\n",
            "   micro avg       0.93      0.95      0.94     76492\n",
            "   macro avg       0.93      0.95      0.94     76492\n",
            "weighted avg       0.93      0.95      0.94     76492\n",
            "\n",
            "02/21/2021 10:38:22 - INFO - __main__ -   ***** Running evaluation on test dataset (5000 step) *****\n",
            "02/21/2021 10:38:22 - INFO - __main__ -     Num examples = 37405\n",
            "02/21/2021 10:38:22 - INFO - __main__ -     Eval Batch size = 128\n",
            "02/21/2021 10:43:21 - INFO - __main__ -   ***** Eval results on test dataset *****\n",
            "02/21/2021 10:43:21 - INFO - __main__ -     f1 = 0.9415486748612624\n",
            "02/21/2021 10:43:21 - INFO - __main__ -     loss = 0.046225458224631415\n",
            "02/21/2021 10:43:21 - INFO - __main__ -     precision = 0.9338957765317146\n",
            "02/21/2021 10:43:21 - INFO - __main__ -     recall = 0.949328034304241\n",
            "02/21/2021 10:43:28 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          DT       0.96      0.98      0.97     13795\n",
            "          LC       0.91      0.92      0.91     12157\n",
            "          OG       0.89      0.91      0.90     18454\n",
            "          PS       0.97      0.98      0.97     14649\n",
            "          QT       0.95      0.97      0.96     16082\n",
            "          TI       0.89      0.95      0.92      1355\n",
            "\n",
            "   micro avg       0.93      0.95      0.94     76492\n",
            "   macro avg       0.93      0.95      0.94     76492\n",
            "weighted avg       0.93      0.95      0.94     76492\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M3emaktOYlk",
        "outputId": "2b666709-cc77-4909-f9e0-1e2871415ff7"
      },
      "source": [
        "import os\n",
        "import json\n",
        "from transformers import ElectraTokenizer, ElectraForTokenClassification  #다른 모델을 쓸 때는 tokenizer(ex.ElectraTokenizer)와 model(ex.ElectraForTokenClassification)를 바꿔주어야 합니다.\n",
        "\n",
        "which_model = input('Which model do you use? ')\n",
        "CONFIG_PATH = os.path.join('./config', 'custom-ner', f'{which_model}.json')\n",
        "\n",
        "with open(CONFIG_PATH, 'r') as f:\n",
        "    model_config = json.load(f)\n",
        "\n",
        "which_tokenizer = model_config['model_name_or_path']\n",
        "\n",
        "tokenizer = ElectraTokenizer.from_pretrained(which_tokenizer)  #tokenizer를 바꾸었다면 from_pretrained에 들어가는 부분을 바꿔주어야 합니다. model config에 있는 model 주소를 적어주시면 됩니다. ex. monologg/koelectra-base-v3-discriminator\n",
        "\n",
        "MODEL_PATH = os.path.join('./ckpt', model_config['output_dir'], 'checkpoint-5000')\n",
        "model = ElectraForTokenClassification.from_pretrained(MODEL_PATH)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Which model do you use? kobert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ./ckpt/kobert-custom-ner-ckpt/checkpoint-5000 were not used when initializing ElectraForTokenClassification: ['bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at ./ckpt/kobert-custom-ner-ckpt/checkpoint-5000 and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'embeddings_project.weight', 'embeddings_project.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Up3mkRRS_7a"
      },
      "source": [
        "label_list=['DT-B','DT-I','LC-B','LC-I','OG-B','OG-I','PS-B','PS-I','QT-B','QT-I','TI-B','TI-I','O']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUVaNlH6Lwob"
      },
      "source": [
        "Kaggle Competition page에서 \"test.txt\"를 다운 받아서 \"data/custom-ner\"에 넣어줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAgE7tEJ2X9M"
      },
      "source": [
        "texts=[]\n",
        "with open(\"data/custom-ner/test.txt\") as f:\n",
        "  for line in f:\n",
        "    texts.append(line.rsplit(\"\\n\")[0])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XrIHaoTItGn",
        "outputId": "d99a3db2-b85c-4b66-ae8f-f41ec945141e"
      },
      "source": [
        "len(texts), texts[:5]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7000,\n",
              " ['인터넷언론 이데일리 출신인 이숙현 안랩 커뮤니케이션팀 부장은 부대변인에 인선됐다',\n",
              "  '엔제리너스의 이벤트 소식이 온라인을 달구고 있는 가운데 트위터로 전해진 다양한 드립을 모아봤다',\n",
              "  '사건 직후 A 씨는 매장 직원의 신고로 출동한 경찰관에 의해 현지 숙소에서 신분 확인을 위한 조사를 받은 것으로 알려졌다',\n",
              "  '경찰에 따르면 조씨 등은 지난해 1월부터 최근까지 서울 용산강남중구의 오피스텔아파트 등지에서 외국인 남성 관광객이 2030대 남성과 성관계 또는 유사성행위를 하도록 주선하고 그 대가로 10만20만원씩 총 6억여원을 챙긴 혐의를 받고 있다',\n",
              "  '경찰에 따르면 A 씨는 24일 밤 10시 21분경 군산시내 한 음식점 앞에서 17살 B 군을 흉기로 한 차례 찔러 살해한 혐의다'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nL396TsMqCL"
      },
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwcpAaql-b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "fd9e889ea21e468ea6ea7d0ce5a4d7b1",
            "107c79cb340347ae8ee3cc1838647e43",
            "280ae71c74e54f9b993c5fbb435a7dc7",
            "0929226c48454c808012ac37a4a18588",
            "4819ddbc7f8541e29da2c5cad8a9a0fa",
            "dc7a1c9a56f6406ab0a6870df77179e6",
            "e74424b2ecce4c89bfbb452f0a679855",
            "bba48383346e4f7584c9ad7e331b0f4f"
          ]
        },
        "outputId": "6c505684-6e72-427d-e084-6351344eb37c"
      },
      "source": [
        "labels=[]\n",
        "for idx, sequence in tqdm(enumerate(texts)):\n",
        "\n",
        "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
        "    inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "    outputs = model(inputs)\n",
        "    predictions = torch.argmax(outputs[0], dim=2)\n",
        "    labels_=[]\n",
        "    for i,j in zip(tokens,predictions[0].tolist()):\n",
        "        if i in ['[CLS]', '[SEP]']:\n",
        "            continue\n",
        "        if \"##\" in i:\n",
        "            continue\n",
        "        labels_.append(label_list[j])\n",
        "    labels.extend(labels_)  "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd9e889ea21e468ea6ea7d0ce5a4d7b1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk5aSkwTMsu7"
      },
      "source": [
        "\"submission.csv\"를 kaggle competition에 제출합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmylkRmjNB65"
      },
      "source": [
        "데이터 전처리,모델 config 튜닝을 이용하여 더 좋은 성적을 얻어보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt6WuSK_h75C"
      },
      "source": [
        "with open(\"submission.csv\",\"w\") as f:\n",
        "  f.write(\"id,tag\\n\")\n",
        "  for idx,tag in enumerate(labels):\n",
        "    f.write(str(idx))\n",
        "    f.write(\",\")\n",
        "    f.write(tag)\n",
        "    f.write(\"\\n\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BczPAw-inat"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}