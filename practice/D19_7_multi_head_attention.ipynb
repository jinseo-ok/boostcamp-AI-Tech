{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7_multi_head_attention.ipynb의 사본","provenance":[{"file_id":"1Yp0oRPYUX7-9XISzhkUBd-18UiE7nsr_","timestamp":1613639418375}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9KsBGZpKkWki"},"source":["##**7. Multi-head Attention**\r\n","1. Multi-head attention 및 self-attention 구현.\r\n","2. 각 과정에서 일어나는 연산과 input/output 형태 이해."]},{"cell_type":"markdown","metadata":{"id":"8qRU5DFY2OM8"},"source":["### **필요 패키지 import**"]},{"cell_type":"code","metadata":{"id":"lDtMioSQQ1bB","executionInfo":{"status":"ok","timestamp":1613639440131,"user_tz":-540,"elapsed":3630,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}}},"source":["from torch import nn\r\n","from torch.nn import functional as F\r\n","from tqdm import tqdm\r\n","\r\n","import torch\r\n","import math"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QBiZObgRep_Q"},"source":["### **데이터 전처리**"]},{"cell_type":"code","metadata":{"id":"e9ULZIqTenSc","executionInfo":{"status":"ok","timestamp":1613639469754,"user_tz":-540,"elapsed":787,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}}},"source":["pad_id = 0\r\n","vocab_size = 100\r\n","\r\n","data = [\r\n","  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\r\n","  [60, 96, 51, 32, 90],\r\n","  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\r\n","  [75, 51],\r\n","  [66, 88, 98, 47],\r\n","  [21, 39, 10, 64, 21],\r\n","  [98],\r\n","  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\r\n","  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\r\n","  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\r\n","]"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Hx3mcivgMyH","executionInfo":{"status":"ok","timestamp":1613639459365,"user_tz":-540,"elapsed":688,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}}},"source":["def padding(data):\r\n","  max_len = len(max(data, key=len)) # 가장 긴 데이터 길이\r\n","  print(f\"Maximum sequence length: {max_len}\")\r\n","\r\n","  for i, seq in enumerate(tqdm(data)):\r\n","    if len(seq) < max_len:\r\n","      data[i] = seq + [pad_id] * (max_len - len(seq)) # 패딩 채워주기\r\n","\r\n","  return data, max_len"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"s3e8FiNvgX60","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613639511483,"user_tz":-540,"elapsed":653,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}},"outputId":"fa52dbc1-0403-4f9b-cb25-00dadcb77ba6"},"source":["data, max_len = padding(data)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["100%|██████████| 10/10 [00:00<00:00, 10240.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Maximum sequence length: 20\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hwPSIWYugaN0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613639512978,"user_tz":-540,"elapsed":1141,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}},"outputId":"06470df1-5755-4467-93ca-ccd56a47c9d5"},"source":["data"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n"," [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [77,\n","  65,\n","  51,\n","  77,\n","  19,\n","  15,\n","  35,\n","  19,\n","  23,\n","  97,\n","  50,\n","  46,\n","  53,\n","  42,\n","  45,\n","  91,\n","  66,\n","  3,\n","  43,\n","  10],\n"," [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n"," [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"rwqjACx8iidc"},"source":["### **Hyperparameter 세팅 및 embedding**"]},{"cell_type":"code","metadata":{"id":"p-Ngp2nWimS8","executionInfo":{"status":"ok","timestamp":1613639516864,"user_tz":-540,"elapsed":683,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}}},"source":["d_model = 512  # model의 hidden size\r\n","num_heads = 8  # head의 개수"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJMi2Xsni5uq","executionInfo":{"status":"ok","timestamp":1613639627049,"user_tz":-540,"elapsed":923,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}}},"source":["# emb layer\r\n","embedding = nn.Embedding(vocab_size, d_model) # vocab_size = 전체 vocab 개수, d_model = emb 차원\r\n","\r\n","# B: batch size, L: maximum sequence length\r\n","batch = torch.LongTensor(data)  # (B, L)\r\n","batch_emb = embedding(batch)  # (B, L, d_model)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"3tLCUQwojcUb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613639627874,"user_tz":-540,"elapsed":1351,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}},"outputId":"3e775151-334f-4d07-96bf-98c50fb2960c"},"source":["print(batch_emb)\r\n","print(batch_emb.shape) # (10, 20, 512) -> 10개의 데이터가 20개의 길이를 가지고 있고 각 토큰은 512 차원을 가지고 있음"],"execution_count":15,"outputs":[{"output_type":"stream","text":["tensor([[[ 0.1662,  0.3838,  2.4579,  ...,  0.2768,  0.3098, -0.7172],\n","         [ 1.4068, -2.4069, -0.2465,  ...,  0.8217, -1.0459,  0.7447],\n","         [-0.0828,  0.4877, -0.2899,  ..., -1.2002,  0.3610, -0.5172],\n","         ...,\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074],\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074],\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074]],\n","\n","        [[-1.5674, -1.6302, -1.1374,  ...,  0.5802, -0.4963, -1.2123],\n","         [-1.4222,  1.3504, -1.8712,  ...,  0.9889, -0.2262,  0.4924],\n","         [ 0.0355, -0.9395,  1.0601,  ..., -0.6091,  1.2678,  1.9001],\n","         ...,\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074],\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074],\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074]],\n","\n","        [[-0.3575,  0.3955,  0.8756,  ...,  1.3762,  0.9230, -1.8228],\n","         [ 0.4443, -0.9241, -0.1414,  ...,  1.3748, -0.2227, -1.3745],\n","         [-0.9007, -1.4022, -0.2368,  ...,  1.1267, -1.3430,  0.3035],\n","         ...,\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074],\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074],\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074]],\n","\n","        ...,\n","\n","        [[ 0.6704, -0.9168,  0.4861,  ...,  0.1210,  0.4177,  1.2168],\n","         [-0.5855,  0.0774,  0.2796,  ...,  1.0991, -0.8384, -1.2712],\n","         [ 0.0355, -0.9395,  1.0601,  ..., -0.6091,  1.2678,  1.9001],\n","         ...,\n","         [-0.4406,  0.4217, -1.4427,  ...,  0.9475,  0.5875,  1.1916],\n","         [ 1.1265, -0.4500,  0.2902,  ..., -0.6250, -0.2915, -0.7451],\n","         [ 0.3225,  0.3052,  0.3335,  ...,  0.0481, -1.2772, -1.0639]],\n","\n","        [[-0.4619,  1.3890, -0.4469,  ...,  0.7843, -1.1870,  1.6207],\n","         [-0.4784,  0.5872,  0.1514,  ..., -0.4834, -1.3693, -0.0774],\n","         [-1.0909, -1.2499, -0.4117,  ..., -0.1385,  1.6212, -1.5496],\n","         ...,\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074],\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074],\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074]],\n","\n","        [[ 0.6354,  1.1014,  0.1886,  ..., -0.5894,  0.3876, -0.4341],\n","         [-0.4784,  0.5872,  0.1514,  ..., -0.4834, -1.3693, -0.0774],\n","         [ 0.8508,  1.8391,  0.4756,  ...,  0.0465, -2.1069,  0.5147],\n","         ...,\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074],\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074],\n","         [-0.6203,  0.5518,  1.4681,  ..., -0.6171,  0.1487,  2.0074]]],\n","       grad_fn=<EmbeddingBackward>)\n","torch.Size([10, 20, 512])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s0Lhx892gmi3"},"source":["### **Linear transformation & 여러 head로 나누기**"]},{"cell_type":"markdown","metadata":{"id":"urXMBRnRgqvw"},"source":["Multi-head attention 내에서 쓰이는 linear transformation matrix들을 정의합니다."]},{"cell_type":"code","metadata":{"id":"9DWKDqgCgfMk","executionInfo":{"status":"ok","timestamp":1613639707294,"user_tz":-540,"elapsed":647,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}}},"source":["w_q = nn.Linear(d_model, d_model)\r\n","w_k = nn.Linear(d_model, d_model)\r\n","w_v = nn.Linear(d_model, d_model)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"tcLuhda7m-Lm","executionInfo":{"status":"ok","timestamp":1613639724821,"user_tz":-540,"elapsed":698,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}}},"source":["w_0 = nn.Linear(d_model, d_model) # "],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-vSL7PwnV6k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613639725085,"user_tz":-540,"elapsed":655,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}},"outputId":"36890ea9-3e76-4c63-cfda-ea0ae6e1ec40"},"source":["# batch_emb.size() = (10, 20, 512)\r\n","q = w_q(batch_emb)  # (B, L, d_model)\r\n","k = w_k(batch_emb)  # (B, L, d_model)\r\n","v = w_v(batch_emb)  # (B, L, d_model)\r\n","\r\n","print(q.shape)\r\n","print(k.shape)\r\n","print(v.shape)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["torch.Size([10, 20, 512])\n","torch.Size([10, 20, 512])\n","torch.Size([10, 20, 512])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wnvlum-LnF1T"},"source":["Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."]},{"cell_type":"code","metadata":{"id":"_tiOKAv9nEli","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613639926144,"user_tz":-540,"elapsed":745,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}},"outputId":"823ab5fd-7603-43e7-e169-3c0045ac180b"},"source":["# num_heads = 8\r\n","batch_size = q.shape[0]\r\n","d_k = d_model // num_heads\r\n","\r\n","q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n","k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n","v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n","\r\n","print(q.shape)\r\n","print(k.shape)\r\n","print(v.shape)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["torch.Size([10, 20, 8, 64])\n","torch.Size([10, 20, 8, 64])\n","torch.Size([10, 20, 8, 64])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5tNb2isfn5Cx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613639971179,"user_tz":-540,"elapsed":910,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}},"outputId":"95a897ff-56c6-40bc-966d-c8b295ee73e7"},"source":["q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n","k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n","v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n","\r\n","print(q.shape)\r\n","print(k.shape)\r\n","print(v.shape)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["torch.Size([10, 8, 20, 64])\n","torch.Size([10, 8, 20, 64])\n","torch.Size([10, 8, 20, 64])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NWrDA5_Sofad"},"source":["### **Scaled dot-product self-attention 구현**"]},{"cell_type":"markdown","metadata":{"id":"w52C4k3Wfl8m"},"source":["각 head에서 실행되는 self-attetion 과정입니다."]},{"cell_type":"code","metadata":{"id":"A5waKr0Hfi2K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613640085079,"user_tz":-540,"elapsed":674,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}},"outputId":"c300e09c-80a3-4c03-9d60-4d70be3012a2"},"source":["# 10개의 데이터가 존재하는 8개의 head의 (20 * 64) dot (64 * 20)\r\n","attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\r\n","attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n","\r\n","print(attn_dists)\r\n","print(attn_dists.shape)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["tensor([[[[0.0489, 0.0607, 0.0438,  ..., 0.0750, 0.0750, 0.0750],\n","          [0.0513, 0.0421, 0.0390,  ..., 0.0581, 0.0581, 0.0581],\n","          [0.0638, 0.0425, 0.0495,  ..., 0.0432, 0.0432, 0.0432],\n","          ...,\n","          [0.0502, 0.0530, 0.0599,  ..., 0.0377, 0.0377, 0.0377],\n","          [0.0502, 0.0530, 0.0599,  ..., 0.0377, 0.0377, 0.0377],\n","          [0.0502, 0.0530, 0.0599,  ..., 0.0377, 0.0377, 0.0377]],\n","\n","         [[0.0497, 0.0355, 0.0502,  ..., 0.0544, 0.0544, 0.0544],\n","          [0.0725, 0.0616, 0.0459,  ..., 0.0627, 0.0627, 0.0627],\n","          [0.0421, 0.0309, 0.0679,  ..., 0.0679, 0.0679, 0.0679],\n","          ...,\n","          [0.0479, 0.0736, 0.0361,  ..., 0.0455, 0.0455, 0.0455],\n","          [0.0479, 0.0736, 0.0361,  ..., 0.0455, 0.0455, 0.0455],\n","          [0.0479, 0.0736, 0.0361,  ..., 0.0455, 0.0455, 0.0455]],\n","\n","         [[0.0701, 0.0398, 0.0367,  ..., 0.0546, 0.0546, 0.0546],\n","          [0.0520, 0.0305, 0.0380,  ..., 0.0591, 0.0591, 0.0591],\n","          [0.0575, 0.0413, 0.0418,  ..., 0.0640, 0.0640, 0.0640],\n","          ...,\n","          [0.0912, 0.0340, 0.0489,  ..., 0.0472, 0.0472, 0.0472],\n","          [0.0912, 0.0340, 0.0489,  ..., 0.0472, 0.0472, 0.0472],\n","          [0.0912, 0.0340, 0.0489,  ..., 0.0472, 0.0472, 0.0472]],\n","\n","         ...,\n","\n","         [[0.0301, 0.0539, 0.0308,  ..., 0.0744, 0.0744, 0.0744],\n","          [0.0277, 0.0534, 0.0517,  ..., 0.0486, 0.0486, 0.0486],\n","          [0.0466, 0.0645, 0.0435,  ..., 0.0543, 0.0543, 0.0543],\n","          ...,\n","          [0.0444, 0.0623, 0.0450,  ..., 0.0582, 0.0582, 0.0582],\n","          [0.0444, 0.0623, 0.0450,  ..., 0.0582, 0.0582, 0.0582],\n","          [0.0444, 0.0623, 0.0450,  ..., 0.0582, 0.0582, 0.0582]],\n","\n","         [[0.0660, 0.0495, 0.0373,  ..., 0.0426, 0.0426, 0.0426],\n","          [0.0439, 0.0595, 0.0372,  ..., 0.0378, 0.0378, 0.0378],\n","          [0.0910, 0.0410, 0.0315,  ..., 0.0300, 0.0300, 0.0300],\n","          ...,\n","          [0.0658, 0.0440, 0.0675,  ..., 0.0386, 0.0386, 0.0386],\n","          [0.0658, 0.0440, 0.0675,  ..., 0.0386, 0.0386, 0.0386],\n","          [0.0658, 0.0440, 0.0675,  ..., 0.0386, 0.0386, 0.0386]],\n","\n","         [[0.0288, 0.0361, 0.0515,  ..., 0.0551, 0.0551, 0.0551],\n","          [0.0341, 0.0422, 0.0363,  ..., 0.0632, 0.0632, 0.0632],\n","          [0.0432, 0.0425, 0.0439,  ..., 0.0410, 0.0410, 0.0410],\n","          ...,\n","          [0.0830, 0.0514, 0.0376,  ..., 0.0557, 0.0557, 0.0557],\n","          [0.0830, 0.0514, 0.0376,  ..., 0.0557, 0.0557, 0.0557],\n","          [0.0830, 0.0514, 0.0376,  ..., 0.0557, 0.0557, 0.0557]]],\n","\n","\n","        [[[0.0524, 0.0462, 0.0800,  ..., 0.0489, 0.0489, 0.0489],\n","          [0.0432, 0.0474, 0.0642,  ..., 0.0502, 0.0502, 0.0502],\n","          [0.0408, 0.0421, 0.0336,  ..., 0.0536, 0.0536, 0.0536],\n","          ...,\n","          [0.0690, 0.0472, 0.0363,  ..., 0.0452, 0.0452, 0.0452],\n","          [0.0690, 0.0472, 0.0363,  ..., 0.0452, 0.0452, 0.0452],\n","          [0.0690, 0.0472, 0.0363,  ..., 0.0452, 0.0452, 0.0452]],\n","\n","         [[0.0608, 0.0420, 0.0445,  ..., 0.0470, 0.0470, 0.0470],\n","          [0.0429, 0.0333, 0.0873,  ..., 0.0501, 0.0501, 0.0501],\n","          [0.0419, 0.0472, 0.0225,  ..., 0.0519, 0.0519, 0.0519],\n","          ...,\n","          [0.0318, 0.0608, 0.0356,  ..., 0.0502, 0.0502, 0.0502],\n","          [0.0318, 0.0608, 0.0356,  ..., 0.0502, 0.0502, 0.0502],\n","          [0.0318, 0.0608, 0.0356,  ..., 0.0502, 0.0502, 0.0502]],\n","\n","         [[0.0569, 0.0339, 0.0571,  ..., 0.0519, 0.0519, 0.0519],\n","          [0.0646, 0.0427, 0.0563,  ..., 0.0488, 0.0488, 0.0488],\n","          [0.0457, 0.0403, 0.0940,  ..., 0.0480, 0.0480, 0.0480],\n","          ...,\n","          [0.1191, 0.0316, 0.0554,  ..., 0.0471, 0.0471, 0.0471],\n","          [0.1191, 0.0316, 0.0554,  ..., 0.0471, 0.0471, 0.0471],\n","          [0.1191, 0.0316, 0.0554,  ..., 0.0471, 0.0471, 0.0471]],\n","\n","         ...,\n","\n","         [[0.0857, 0.0407, 0.0743,  ..., 0.0433, 0.0433, 0.0433],\n","          [0.0302, 0.0646, 0.0382,  ..., 0.0530, 0.0530, 0.0530],\n","          [0.0210, 0.0457, 0.0650,  ..., 0.0529, 0.0529, 0.0529],\n","          ...,\n","          [0.0353, 0.0586, 0.0309,  ..., 0.0528, 0.0528, 0.0528],\n","          [0.0353, 0.0586, 0.0309,  ..., 0.0528, 0.0528, 0.0528],\n","          [0.0353, 0.0586, 0.0309,  ..., 0.0528, 0.0528, 0.0528]],\n","\n","         [[0.0638, 0.0586, 0.0695,  ..., 0.0457, 0.0457, 0.0457],\n","          [0.0581, 0.0221, 0.0374,  ..., 0.0515, 0.0515, 0.0515],\n","          [0.0722, 0.0631, 0.0457,  ..., 0.0486, 0.0486, 0.0486],\n","          ...,\n","          [0.0628, 0.0568, 0.0556,  ..., 0.0502, 0.0502, 0.0502],\n","          [0.0628, 0.0568, 0.0556,  ..., 0.0502, 0.0502, 0.0502],\n","          [0.0628, 0.0568, 0.0556,  ..., 0.0502, 0.0502, 0.0502]],\n","\n","         [[0.0456, 0.0915, 0.1487,  ..., 0.0386, 0.0386, 0.0386],\n","          [0.1344, 0.0437, 0.0396,  ..., 0.0441, 0.0441, 0.0441],\n","          [0.0384, 0.0266, 0.0423,  ..., 0.0541, 0.0541, 0.0541],\n","          ...,\n","          [0.0341, 0.0260, 0.0380,  ..., 0.0521, 0.0521, 0.0521],\n","          [0.0341, 0.0260, 0.0380,  ..., 0.0521, 0.0521, 0.0521],\n","          [0.0341, 0.0260, 0.0380,  ..., 0.0521, 0.0521, 0.0521]]],\n","\n","\n","        [[[0.0700, 0.0443, 0.0749,  ..., 0.0411, 0.0411, 0.0411],\n","          [0.0417, 0.0388, 0.0783,  ..., 0.0448, 0.0448, 0.0448],\n","          [0.0350, 0.0351, 0.0285,  ..., 0.0636, 0.0636, 0.0636],\n","          ...,\n","          [0.0521, 0.0457, 0.0392,  ..., 0.0436, 0.0436, 0.0436],\n","          [0.0521, 0.0457, 0.0392,  ..., 0.0436, 0.0436, 0.0436],\n","          [0.0521, 0.0457, 0.0392,  ..., 0.0436, 0.0436, 0.0436]],\n","\n","         [[0.0759, 0.0988, 0.0397,  ..., 0.0415, 0.0415, 0.0415],\n","          [0.0481, 0.0476, 0.0426,  ..., 0.0586, 0.0586, 0.0586],\n","          [0.0681, 0.0245, 0.0693,  ..., 0.0451, 0.0451, 0.0451],\n","          ...,\n","          [0.0349, 0.0529, 0.0384,  ..., 0.0415, 0.0415, 0.0415],\n","          [0.0349, 0.0529, 0.0384,  ..., 0.0415, 0.0415, 0.0415],\n","          [0.0349, 0.0529, 0.0384,  ..., 0.0415, 0.0415, 0.0415]],\n","\n","         [[0.0559, 0.0980, 0.0455,  ..., 0.0335, 0.0335, 0.0335],\n","          [0.0314, 0.0387, 0.0552,  ..., 0.0619, 0.0619, 0.0619],\n","          [0.0610, 0.0585, 0.0736,  ..., 0.0341, 0.0341, 0.0341],\n","          ...,\n","          [0.0913, 0.0341, 0.0327,  ..., 0.0444, 0.0444, 0.0444],\n","          [0.0913, 0.0341, 0.0327,  ..., 0.0444, 0.0444, 0.0444],\n","          [0.0913, 0.0341, 0.0327,  ..., 0.0444, 0.0444, 0.0444]],\n","\n","         ...,\n","\n","         [[0.0413, 0.0501, 0.0593,  ..., 0.0495, 0.0495, 0.0495],\n","          [0.0394, 0.0607, 0.0350,  ..., 0.0556, 0.0556, 0.0556],\n","          [0.0646, 0.0514, 0.0242,  ..., 0.0547, 0.0547, 0.0547],\n","          ...,\n","          [0.0282, 0.0216, 0.0418,  ..., 0.0633, 0.0633, 0.0633],\n","          [0.0282, 0.0216, 0.0418,  ..., 0.0633, 0.0633, 0.0633],\n","          [0.0282, 0.0216, 0.0418,  ..., 0.0633, 0.0633, 0.0633]],\n","\n","         [[0.0442, 0.0618, 0.1019,  ..., 0.0329, 0.0329, 0.0329],\n","          [0.0455, 0.0506, 0.0589,  ..., 0.0521, 0.0521, 0.0521],\n","          [0.0468, 0.0551, 0.0298,  ..., 0.0419, 0.0419, 0.0419],\n","          ...,\n","          [0.0383, 0.0428, 0.0391,  ..., 0.0396, 0.0396, 0.0396],\n","          [0.0383, 0.0428, 0.0391,  ..., 0.0396, 0.0396, 0.0396],\n","          [0.0383, 0.0428, 0.0391,  ..., 0.0396, 0.0396, 0.0396]],\n","\n","         [[0.0558, 0.0959, 0.0476,  ..., 0.0409, 0.0409, 0.0409],\n","          [0.0393, 0.0393, 0.0585,  ..., 0.0600, 0.0600, 0.0600],\n","          [0.0287, 0.0413, 0.0671,  ..., 0.0524, 0.0524, 0.0524],\n","          ...,\n","          [0.0507, 0.0185, 0.0453,  ..., 0.0613, 0.0613, 0.0613],\n","          [0.0507, 0.0185, 0.0453,  ..., 0.0613, 0.0613, 0.0613],\n","          [0.0507, 0.0185, 0.0453,  ..., 0.0613, 0.0613, 0.0613]]],\n","\n","\n","        ...,\n","\n","\n","        [[[0.0521, 0.0429, 0.0333,  ..., 0.0586, 0.0407, 0.0498],\n","          [0.0377, 0.0432, 0.0378,  ..., 0.0677, 0.0659, 0.0597],\n","          [0.0598, 0.0554, 0.0450,  ..., 0.0500, 0.0384, 0.0475],\n","          ...,\n","          [0.0590, 0.0273, 0.0440,  ..., 0.0545, 0.0475, 0.0580],\n","          [0.0556, 0.0907, 0.0691,  ..., 0.0468, 0.0365, 0.0265],\n","          [0.0307, 0.0462, 0.0465,  ..., 0.0298, 0.0350, 0.0443]],\n","\n","         [[0.0537, 0.0583, 0.0285,  ..., 0.0431, 0.0626, 0.0542],\n","          [0.0668, 0.0423, 0.0342,  ..., 0.0415, 0.0392, 0.0794],\n","          [0.0262, 0.0677, 0.0221,  ..., 0.0644, 0.0316, 0.0365],\n","          ...,\n","          [0.0554, 0.0415, 0.0466,  ..., 0.0582, 0.0549, 0.0341],\n","          [0.0621, 0.0391, 0.0354,  ..., 0.0606, 0.0293, 0.0521],\n","          [0.0659, 0.0463, 0.0419,  ..., 0.0348, 0.0605, 0.0488]],\n","\n","         [[0.0436, 0.0606, 0.0674,  ..., 0.0474, 0.0570, 0.0762],\n","          [0.0274, 0.0433, 0.0626,  ..., 0.0440, 0.0369, 0.0645],\n","          [0.0398, 0.0530, 0.0749,  ..., 0.0528, 0.0281, 0.0685],\n","          ...,\n","          [0.0404, 0.0777, 0.0436,  ..., 0.0549, 0.0474, 0.0391],\n","          [0.0224, 0.0489, 0.0390,  ..., 0.0426, 0.0285, 0.0570],\n","          [0.0898, 0.0575, 0.0564,  ..., 0.0425, 0.0436, 0.0431]],\n","\n","         ...,\n","\n","         [[0.0416, 0.0628, 0.0374,  ..., 0.0966, 0.0502, 0.0621],\n","          [0.0509, 0.0586, 0.0445,  ..., 0.0531, 0.0387, 0.0614],\n","          [0.0477, 0.0561, 0.0816,  ..., 0.0292, 0.0342, 0.0411],\n","          ...,\n","          [0.0743, 0.0319, 0.0490,  ..., 0.0695, 0.0586, 0.0501],\n","          [0.0371, 0.0322, 0.0557,  ..., 0.0465, 0.0407, 0.0646],\n","          [0.0403, 0.0545, 0.0685,  ..., 0.0439, 0.0447, 0.0544]],\n","\n","         [[0.0454, 0.0623, 0.0431,  ..., 0.0387, 0.0289, 0.0620],\n","          [0.0477, 0.0620, 0.0609,  ..., 0.0446, 0.0498, 0.0441],\n","          [0.0637, 0.0540, 0.0452,  ..., 0.0447, 0.0393, 0.0587],\n","          ...,\n","          [0.0666, 0.0328, 0.0463,  ..., 0.0533, 0.0304, 0.0581],\n","          [0.0318, 0.0660, 0.0438,  ..., 0.0564, 0.1282, 0.0271],\n","          [0.0517, 0.0560, 0.0617,  ..., 0.0428, 0.0311, 0.1100]],\n","\n","         [[0.0271, 0.0559, 0.0552,  ..., 0.0476, 0.0402, 0.0637],\n","          [0.0439, 0.0445, 0.0512,  ..., 0.0513, 0.0394, 0.0444],\n","          [0.0443, 0.0233, 0.0669,  ..., 0.0478, 0.0623, 0.0345],\n","          ...,\n","          [0.0498, 0.0433, 0.0572,  ..., 0.0354, 0.0525, 0.0355],\n","          [0.0516, 0.0831, 0.0342,  ..., 0.0566, 0.0344, 0.0371],\n","          [0.0484, 0.0472, 0.0364,  ..., 0.0546, 0.0822, 0.0302]]],\n","\n","\n","        [[[0.0405, 0.0397, 0.0477,  ..., 0.0483, 0.0483, 0.0483],\n","          [0.0335, 0.0702, 0.0425,  ..., 0.0585, 0.0585, 0.0585],\n","          [0.0405, 0.0838, 0.0627,  ..., 0.0354, 0.0354, 0.0354],\n","          ...,\n","          [0.0722, 0.0720, 0.0847,  ..., 0.0402, 0.0402, 0.0402],\n","          [0.0722, 0.0720, 0.0847,  ..., 0.0402, 0.0402, 0.0402],\n","          [0.0722, 0.0720, 0.0847,  ..., 0.0402, 0.0402, 0.0402]],\n","\n","         [[0.0496, 0.0499, 0.0544,  ..., 0.0526, 0.0526, 0.0526],\n","          [0.0393, 0.0810, 0.0636,  ..., 0.0556, 0.0556, 0.0556],\n","          [0.0302, 0.0527, 0.0326,  ..., 0.0356, 0.0356, 0.0356],\n","          ...,\n","          [0.0419, 0.0600, 0.0850,  ..., 0.0450, 0.0450, 0.0450],\n","          [0.0419, 0.0600, 0.0850,  ..., 0.0450, 0.0450, 0.0450],\n","          [0.0419, 0.0600, 0.0850,  ..., 0.0450, 0.0450, 0.0450]],\n","\n","         [[0.0440, 0.0339, 0.0707,  ..., 0.0705, 0.0705, 0.0705],\n","          [0.0610, 0.0308, 0.0426,  ..., 0.0647, 0.0647, 0.0647],\n","          [0.0686, 0.0427, 0.0446,  ..., 0.0388, 0.0388, 0.0388],\n","          ...,\n","          [0.0542, 0.0406, 0.0417,  ..., 0.0437, 0.0437, 0.0437],\n","          [0.0542, 0.0406, 0.0417,  ..., 0.0437, 0.0437, 0.0437],\n","          [0.0542, 0.0406, 0.0417,  ..., 0.0437, 0.0437, 0.0437]],\n","\n","         ...,\n","\n","         [[0.0321, 0.0425, 0.0434,  ..., 0.0660, 0.0660, 0.0660],\n","          [0.0422, 0.0327, 0.0532,  ..., 0.0332, 0.0332, 0.0332],\n","          [0.0358, 0.0510, 0.0555,  ..., 0.0500, 0.0500, 0.0500],\n","          ...,\n","          [0.0400, 0.0571, 0.0671,  ..., 0.0677, 0.0677, 0.0677],\n","          [0.0400, 0.0571, 0.0671,  ..., 0.0677, 0.0677, 0.0677],\n","          [0.0400, 0.0571, 0.0671,  ..., 0.0677, 0.0677, 0.0677]],\n","\n","         [[0.0482, 0.0463, 0.0406,  ..., 0.0299, 0.0299, 0.0299],\n","          [0.0236, 0.0425, 0.0378,  ..., 0.0779, 0.0779, 0.0779],\n","          [0.0411, 0.0549, 0.0574,  ..., 0.0396, 0.0396, 0.0396],\n","          ...,\n","          [0.0683, 0.0407, 0.0510,  ..., 0.0430, 0.0430, 0.0430],\n","          [0.0683, 0.0407, 0.0510,  ..., 0.0430, 0.0430, 0.0430],\n","          [0.0683, 0.0407, 0.0510,  ..., 0.0430, 0.0430, 0.0430]],\n","\n","         [[0.0225, 0.0393, 0.0416,  ..., 0.0708, 0.0708, 0.0708],\n","          [0.0681, 0.0399, 0.0332,  ..., 0.0355, 0.0355, 0.0355],\n","          [0.0777, 0.0319, 0.0754,  ..., 0.0257, 0.0257, 0.0257],\n","          ...,\n","          [0.0655, 0.0321, 0.0525,  ..., 0.0563, 0.0563, 0.0563],\n","          [0.0655, 0.0321, 0.0525,  ..., 0.0563, 0.0563, 0.0563],\n","          [0.0655, 0.0321, 0.0525,  ..., 0.0563, 0.0563, 0.0563]]],\n","\n","\n","        [[[0.0425, 0.0551, 0.0506,  ..., 0.0377, 0.0377, 0.0377],\n","          [0.0393, 0.0686, 0.0543,  ..., 0.0572, 0.0572, 0.0572],\n","          [0.0276, 0.0383, 0.0531,  ..., 0.0630, 0.0630, 0.0630],\n","          ...,\n","          [0.0559, 0.0705, 0.0844,  ..., 0.0394, 0.0394, 0.0394],\n","          [0.0559, 0.0705, 0.0844,  ..., 0.0394, 0.0394, 0.0394],\n","          [0.0559, 0.0705, 0.0844,  ..., 0.0394, 0.0394, 0.0394]],\n","\n","         [[0.0292, 0.0627, 0.0477,  ..., 0.0477, 0.0477, 0.0477],\n","          [0.0188, 0.0873, 0.0311,  ..., 0.0598, 0.0598, 0.0598],\n","          [0.0400, 0.0557, 0.0593,  ..., 0.0466, 0.0466, 0.0466],\n","          ...,\n","          [0.0597, 0.0594, 0.0330,  ..., 0.0446, 0.0446, 0.0446],\n","          [0.0597, 0.0594, 0.0330,  ..., 0.0446, 0.0446, 0.0446],\n","          [0.0597, 0.0594, 0.0330,  ..., 0.0446, 0.0446, 0.0446]],\n","\n","         [[0.0636, 0.0543, 0.0510,  ..., 0.0375, 0.0375, 0.0375],\n","          [0.0286, 0.0331, 0.0407,  ..., 0.0695, 0.0695, 0.0695],\n","          [0.0338, 0.0929, 0.0898,  ..., 0.0442, 0.0442, 0.0442],\n","          ...,\n","          [0.0401, 0.0408, 0.0325,  ..., 0.0439, 0.0439, 0.0439],\n","          [0.0401, 0.0408, 0.0325,  ..., 0.0439, 0.0439, 0.0439],\n","          [0.0401, 0.0408, 0.0325,  ..., 0.0439, 0.0439, 0.0439]],\n","\n","         ...,\n","\n","         [[0.0706, 0.0427, 0.0309,  ..., 0.0509, 0.0509, 0.0509],\n","          [0.0511, 0.0319, 0.0784,  ..., 0.0324, 0.0324, 0.0324],\n","          [0.0433, 0.0759, 0.0599,  ..., 0.0390, 0.0390, 0.0390],\n","          ...,\n","          [0.0508, 0.0570, 0.0446,  ..., 0.0676, 0.0676, 0.0676],\n","          [0.0508, 0.0570, 0.0446,  ..., 0.0676, 0.0676, 0.0676],\n","          [0.0508, 0.0570, 0.0446,  ..., 0.0676, 0.0676, 0.0676]],\n","\n","         [[0.0469, 0.0321, 0.0662,  ..., 0.0474, 0.0474, 0.0474],\n","          [0.0517, 0.0365, 0.0482,  ..., 0.0669, 0.0669, 0.0669],\n","          [0.0606, 0.0310, 0.0412,  ..., 0.0370, 0.0370, 0.0370],\n","          ...,\n","          [0.1026, 0.0387, 0.0506,  ..., 0.0409, 0.0409, 0.0409],\n","          [0.1026, 0.0387, 0.0506,  ..., 0.0409, 0.0409, 0.0409],\n","          [0.1026, 0.0387, 0.0506,  ..., 0.0409, 0.0409, 0.0409]],\n","\n","         [[0.0490, 0.0316, 0.0642,  ..., 0.0369, 0.0369, 0.0369],\n","          [0.0318, 0.0412, 0.0469,  ..., 0.0366, 0.0366, 0.0366],\n","          [0.0458, 0.0568, 0.0683,  ..., 0.0298, 0.0298, 0.0298],\n","          ...,\n","          [0.0702, 0.0303, 0.0597,  ..., 0.0532, 0.0532, 0.0532],\n","          [0.0702, 0.0303, 0.0597,  ..., 0.0532, 0.0532, 0.0532],\n","          [0.0702, 0.0303, 0.0597,  ..., 0.0532, 0.0532, 0.0532]]]],\n","       grad_fn=<SoftmaxBackward>)\n","torch.Size([10, 8, 20, 20])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7megouWpgCck","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613640164527,"user_tz":-540,"elapsed":737,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}},"outputId":"eebb0107-ed04-44cf-cea5-51126b82aee6"},"source":["attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\r\n","\r\n","print(attn_values.shape)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["torch.Size([10, 8, 20, 64])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LmSTaymdg-P_"},"source":["### **각 head의 결과물 병합**"]},{"cell_type":"markdown","metadata":{"id":"YSdQZCk0hCNd"},"source":["각 head의 결과물을 concat하고 동일 차원으로 linear transformation합니다."]},{"cell_type":"code","metadata":{"id":"eaK0bpMGhQZ2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613640195691,"user_tz":-540,"elapsed":1130,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}},"outputId":"ca0537c4-be8d-4287-d7d8-7a2de8d75060"},"source":["attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\r\n","attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model) # concat하는 과정인가?\r\n","\r\n","print(attn_values.shape)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["torch.Size([10, 20, 512])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LTng_2SXhdH1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613640365975,"user_tz":-540,"elapsed":867,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}},"outputId":"1c540b9a-e61d-48ca-d5b6-bbeaaf680b32"},"source":["outputs = w_0(attn_values) # Linear transformation 해주는\r\n","\r\n","print(outputs.shape)\r\n","print(outputs)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["torch.Size([10, 20, 512])\n","tensor([[[-0.0258,  0.1026, -0.2086,  ..., -0.1510,  0.2324, -0.0641],\n","         [-0.0064,  0.1259, -0.2044,  ..., -0.1109,  0.2817, -0.0371],\n","         [-0.0137,  0.1545, -0.1765,  ..., -0.0479,  0.2172, -0.0493],\n","         ...,\n","         [-0.0681,  0.0860, -0.2196,  ..., -0.0833,  0.2378, -0.0051],\n","         [-0.0681,  0.0860, -0.2196,  ..., -0.0833,  0.2378, -0.0051],\n","         [-0.0681,  0.0860, -0.2196,  ..., -0.0833,  0.2378, -0.0051]],\n","\n","        [[-0.2378,  0.0199, -0.1467,  ..., -0.4187,  0.3819, -0.3111],\n","         [-0.2250,  0.0102, -0.1895,  ..., -0.4359,  0.4152, -0.3523],\n","         [-0.2186,  0.0431, -0.1793,  ..., -0.4798,  0.4759, -0.3452],\n","         ...,\n","         [-0.1684,  0.0335, -0.1808,  ..., -0.4452,  0.4642, -0.3485],\n","         [-0.1684,  0.0335, -0.1808,  ..., -0.4452,  0.4642, -0.3485],\n","         [-0.1684,  0.0335, -0.1808,  ..., -0.4452,  0.4642, -0.3485]],\n","\n","        [[-0.0591, -0.0127, -0.2687,  ..., -0.2445,  0.2560, -0.2716],\n","         [-0.0912,  0.0186, -0.2493,  ..., -0.3226,  0.3179, -0.3048],\n","         [-0.1539,  0.0189, -0.2849,  ..., -0.3695,  0.3347, -0.2302],\n","         ...,\n","         [-0.0545,  0.0147, -0.2753,  ..., -0.3082,  0.2553, -0.2731],\n","         [-0.0545,  0.0147, -0.2753,  ..., -0.3082,  0.2553, -0.2731],\n","         [-0.0545,  0.0147, -0.2753,  ..., -0.3082,  0.2553, -0.2731]],\n","\n","        ...,\n","\n","        [[-0.0262, -0.0158, -0.1776,  ..., -0.0378,  0.0250,  0.0025],\n","         [ 0.0053,  0.0179, -0.2076,  ...,  0.0047,  0.0623,  0.0085],\n","         [ 0.0508,  0.0384, -0.1255,  ..., -0.0249,  0.0351,  0.0124],\n","         ...,\n","         [-0.0131,  0.0068, -0.1163,  ...,  0.0034,  0.0508,  0.0098],\n","         [ 0.0180,  0.0196, -0.0601,  ..., -0.0156,  0.0661,  0.0068],\n","         [-0.0690, -0.0120, -0.1834,  ..., -0.0592,  0.0586,  0.0580]],\n","\n","        [[-0.0736,  0.0943, -0.1403,  ..., -0.2163,  0.2716, -0.0682],\n","         [-0.1934,  0.0518, -0.0797,  ..., -0.1910,  0.3260, -0.0929],\n","         [-0.1149,  0.1059, -0.0858,  ..., -0.1471,  0.2805, -0.0591],\n","         ...,\n","         [-0.1154,  0.1165, -0.1276,  ..., -0.1781,  0.2744, -0.0776],\n","         [-0.1154,  0.1165, -0.1276,  ..., -0.1781,  0.2744, -0.0776],\n","         [-0.1154,  0.1165, -0.1276,  ..., -0.1781,  0.2744, -0.0776]],\n","\n","        [[-0.1152,  0.0270, -0.1009,  ..., -0.1933,  0.2385, -0.1419],\n","         [-0.1920,  0.0435, -0.0888,  ..., -0.1781,  0.2932, -0.1231],\n","         [-0.1378,  0.0602, -0.1166,  ..., -0.1536,  0.2281, -0.1108],\n","         ...,\n","         [-0.1689,  0.0905, -0.1745,  ..., -0.2094,  0.2828, -0.1768],\n","         [-0.1689,  0.0905, -0.1745,  ..., -0.2094,  0.2828, -0.1768],\n","         [-0.1689,  0.0905, -0.1745,  ..., -0.2094,  0.2828, -0.1768]]],\n","       grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"goX70VKqhxQH"},"source":["### **전체 코드**"]},{"cell_type":"markdown","metadata":{"id":"WtNyV7mMj7V_"},"source":["위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈을 구현하겠습니다."]},{"cell_type":"code","metadata":{"id":"U_kNhOTrkBHm","executionInfo":{"status":"ok","timestamp":1613640382317,"user_tz":-540,"elapsed":962,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}}},"source":["class MultiheadAttention(nn.Module):\r\n","  def __init__(self):\r\n","    super(MultiheadAttention, self).__init__()\r\n","\r\n","    # Q, K, V learnable matrices\r\n","    self.w_q = nn.Linear(d_model, d_model)\r\n","    self.w_k = nn.Linear(d_model, d_model)\r\n","    self.w_v = nn.Linear(d_model, d_model)\r\n","\r\n","    # Linear transformation for concatenated outputs\r\n","    self.w_0 = nn.Linear(d_model, d_model)\r\n","\r\n","  def forward(self, q, k, v):\r\n","    batch_size = q.shape[0]\r\n","\r\n","    q = self.w_q(q)  # (B, L, d_model)\r\n","    k = self.w_k(k)  # (B, L, d_model)\r\n","    v = self.w_v(v)  # (B, L, d_model)\r\n","\r\n","    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n","    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n","    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n","\r\n","    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n","    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n","    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n","\r\n","    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\r\n","    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\r\n","\r\n","    return self.w_0(attn_values)\r\n","\r\n","  def self_attention(self, q, k, v):\r\n","    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\r\n","    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n","\r\n","    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\r\n","\r\n","    return attn_values"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYLuu_9alQxT","executionInfo":{"status":"ok","timestamp":1613640382318,"user_tz":-540,"elapsed":779,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}}},"source":["multihead_attn = MultiheadAttention()\r\n","\r\n","outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMiXlYjSlTfB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613640385874,"user_tz":-540,"elapsed":1135,"user":{"displayName":"오진석","photoUrl":"","userId":"12229120938967938448"}},"outputId":"7f1595d3-3a3d-4f21-f7bc-a4e7150d9632"},"source":["print(outputs.shape)\r\n","print(outputs)"],"execution_count":50,"outputs":[{"output_type":"stream","text":["torch.Size([10, 20, 512])\n","tensor([[[-0.0049,  0.1423,  0.0913,  ...,  0.0972, -0.0080,  0.1448],\n","         [ 0.0450,  0.0733,  0.0623,  ...,  0.0858, -0.0140,  0.1039],\n","         [-0.0068,  0.0650,  0.0804,  ...,  0.0119, -0.0195,  0.0922],\n","         ...,\n","         [-0.0017,  0.0737,  0.1249,  ...,  0.0576, -0.0383,  0.1278],\n","         [-0.0017,  0.0737,  0.1249,  ...,  0.0576, -0.0383,  0.1278],\n","         [-0.0017,  0.0737,  0.1249,  ...,  0.0576, -0.0383,  0.1278]],\n","\n","        [[-0.1339,  0.2719,  0.2828,  ..., -0.2396, -0.1392,  0.3415],\n","         [-0.1135,  0.3218,  0.2687,  ..., -0.2579, -0.0988,  0.3660],\n","         [-0.0815,  0.3035,  0.2919,  ..., -0.2346, -0.0671,  0.3198],\n","         ...,\n","         [-0.1189,  0.2846,  0.2677,  ..., -0.2408, -0.1038,  0.3249],\n","         [-0.1189,  0.2846,  0.2677,  ..., -0.2408, -0.1038,  0.3249],\n","         [-0.1189,  0.2846,  0.2677,  ..., -0.2408, -0.1038,  0.3249]],\n","\n","        [[-0.0894,  0.1179,  0.1524,  ..., -0.1467, -0.0475,  0.2280],\n","         [-0.0531,  0.0798,  0.1489,  ..., -0.0950, -0.0658,  0.1934],\n","         [-0.0780,  0.1453,  0.1305,  ..., -0.1092, -0.0286,  0.1893],\n","         ...,\n","         [-0.0742,  0.1440,  0.1680,  ..., -0.0949, -0.0653,  0.1878],\n","         [-0.0742,  0.1440,  0.1680,  ..., -0.0949, -0.0653,  0.1878],\n","         [-0.0742,  0.1440,  0.1680,  ..., -0.0949, -0.0653,  0.1878]],\n","\n","        ...,\n","\n","        [[ 0.0668,  0.0816, -0.0083,  ...,  0.0117, -0.0252,  0.0146],\n","         [ 0.0334,  0.0379, -0.0135,  ...,  0.0177, -0.0061,  0.0909],\n","         [ 0.0621,  0.0257, -0.0615,  ...,  0.0330,  0.0630,  0.0545],\n","         ...,\n","         [-0.0237,  0.0331, -0.0024,  ...,  0.0519, -0.0151,  0.0557],\n","         [ 0.0291,  0.0168, -0.0279,  ...,  0.0195,  0.0816,  0.0144],\n","         [ 0.0354,  0.0254, -0.0570,  ...,  0.0389,  0.0007,  0.0870]],\n","\n","        [[-0.0166,  0.1308,  0.0174,  ...,  0.0513,  0.1308,  0.1230],\n","         [-0.0487,  0.1559,  0.0660,  ..., -0.0114,  0.0898,  0.1822],\n","         [-0.0283,  0.0892,  0.0508,  ...,  0.0148,  0.0765,  0.1791],\n","         ...,\n","         [-0.0128,  0.1166,  0.0145,  ...,  0.0387,  0.0680,  0.1533],\n","         [-0.0128,  0.1166,  0.0145,  ...,  0.0387,  0.0680,  0.1533],\n","         [-0.0128,  0.1166,  0.0145,  ...,  0.0387,  0.0680,  0.1533]],\n","\n","        [[-0.0466,  0.1678,  0.1262,  ..., -0.0104, -0.0260,  0.1272],\n","         [-0.0814,  0.2300,  0.1433,  ..., -0.0610, -0.0308,  0.1759],\n","         [-0.0770,  0.1862,  0.0739,  ...,  0.0009, -0.0229,  0.1415],\n","         ...,\n","         [-0.0826,  0.1864,  0.0954,  ...,  0.0203, -0.0072,  0.1342],\n","         [-0.0826,  0.1864,  0.0954,  ...,  0.0203, -0.0072,  0.1342],\n","         [-0.0826,  0.1864,  0.0954,  ...,  0.0203, -0.0072,  0.1342]]],\n","       grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2cYIIdvEdjUd"},"source":[""],"execution_count":null,"outputs":[]}]}